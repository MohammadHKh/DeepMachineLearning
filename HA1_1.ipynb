{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5fa9f96d4ca0144b2db877078cf7b2f8",
     "grade": false,
     "grade_id": "cell-5690119ead85e67e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Checklist for submission\n",
    "\n",
    "It is extremely important to make sure that:\n",
    "\n",
    "1. Everything runs as expected (no bugs when running cells);\n",
    "2. The output from each cell corresponds to its code (don't change any cell's contents without rerunning it afterwards);\n",
    "3. All outputs are present (don't delete any of the outputs);\n",
    "4. Fill in all the places that say `# YOUR CODE HERE`, or \"**Your answer:** (fill in here)\".\n",
    "5. Never copy/paste any notebook cells. Inserting new cells is allowed, but it should not be necessary.\n",
    "6. The notebook contains some hidden metadata which is important during our grading process. **Make sure not to corrupt any of this metadata!** The metadata may for example be corrupted if you copy/paste any notebook cells, or if you perform an unsuccessful git merge / git pull. It may also be pruned completely if using Google Colab, so watch out for this. Searching for \"nbgrader\" when opening the notebook in a text editor should take you to the important metadata entries.\n",
    "7. Although we will try our very best to avoid this, it may happen that bugs are found after an assignment is released, and that we will push an updated version of the assignment to GitHub. If this happens, it is important that you update to the new version, while making sure the notebook metadata is properly updated as well. The safest way to make sure nothing gets messed up is to start from scratch on a clean updated version of the notebook, copy/pasting your code from the cells of the previous version into the cells of the new version.\n",
    "8. If you need to have multiple parallel versions of this notebook, make sure not to move them to another directory.\n",
    "9. Although not forced to work exclusively in the course `conda` environment, you need to make sure that the notebook will run in that environment, i.e. that you have not added any additional dependencies.\n",
    "\n",
    "**FOR HA1, HA2, HA3 ONLY:** Failing to meet any of these requirements might lead to either a subtraction of POEs (at best) or a request for resubmission (at worst).\n",
    "\n",
    "We advise you to perform the following steps before submission to ensure that requirements 1, 2, and 3 are always met: **Restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). This might require a bit of time, so plan ahead for this (and possibly use Google Cloud's GPU in HA1 and HA2 for this step). Finally press the \"Save and Checkout\" button before handing in, to make sure that all your changes are saved to this .ipynb file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6bb874a16c1ff767ac0f37ce0491265",
     "grade": false,
     "grade_id": "cell-774c93bf6433de68",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Fill in name of notebook file\n",
    "This might seem silly, but the version check below needs to know the filename of the current notebook, which is not trivial to find out programmatically.\n",
    "\n",
    "You might want to have several parallel versions of the notebook, and it is fine to rename the notebook as long as it stays in the same directory. **However**, if you do rename it, you also need to update its own filename below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_fname = \"XXX.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "879883c2ea755808ffd00aeee5c77a00",
     "grade": false,
     "grade_id": "cell-5676bcf768a7f9be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Fill in group number and member names (use NAME2 and GROUP only for HA1, HA2 and HA3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME1 = \"\" \n",
    "NAME2 = \"\"\n",
    "GROUP = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42f960a95815e1aa3ce8132fcec59cd9",
     "grade": false,
     "grade_id": "cell-a15fe781533d9590",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Check Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a5517d7993b4b35049f0013dd6a3f55",
     "grade": false,
     "grade_id": "cell-2b9c2390ee464c39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from platform import python_version_tuple\n",
    "assert python_version_tuple()[:2] == ('3','7'), \"You are not running Python 3.7. Make sure to run Python through the course Conda environment.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15ec4309f1e85f6e17bda73b9b6f48a2",
     "grade": false,
     "grade_id": "cell-4869b45600ce82f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Check that notebook server has access to all required resources, and that notebook has not moved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2d199303c73ec86d25177caf39e385f",
     "grade": false,
     "grade_id": "cell-122ac3d9100b8afb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "nb_dirname = os.path.abspath('')\n",
    "assignment_name = os.path.basename(nb_dirname)\n",
    "assert assignment_name in ['IHA1', 'IHA2', 'HA1', 'HA2', 'HA3'], \\\n",
    "    '[ERROR] The notebook appears to have been moved from its original directory'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f09f40b5350db83232189137c550f0a1",
     "grade": false,
     "grade_id": "cell-2455deee513cd39c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Verify correct nb_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a78c7227b049bb147e6c363affb6dae8",
     "grade": false,
     "grade_id": "cell-0472e2fd710f1d72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "try:\n",
    "    display(HTML(r'<script>if(\"{nb_fname}\" != IPython.notebook.notebook_name) {{ alert(\"You have filled in nb_fname = \\\"{nb_fname}\\\", but this does not seem to match the notebook filename \\\"\" + IPython.notebook.notebook_name + \"\\\".\"); }}</script>'.format(nb_fname=nb_fname)))\n",
    "except NameError:\n",
    "    assert False, 'Make sure to fill in the nb_fname variable above!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98d88d8e8da19693053764f29dcc591d",
     "grade": false,
     "grade_id": "cell-ceacb1adcae4783d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Verify that your notebook is up-to-date and not corrupted in any way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb700528d4644601c1a8c91ef1d84635",
     "grade": false,
     "grade_id": "cell-f5a59288e11b4aec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from ha_utils import check_notebook_uptodate_and_not_corrupted\n",
    "check_notebook_uptodate_and_not_corrupted(nb_dirname, nb_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f0dd7cbad727dec0308b03071cff6d79",
     "grade": false,
     "grade_id": "cell-8092c3fd452a3245",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# HA1 - Cats and dogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a54241ea89512f794ee9e366f2ef92f3",
     "grade": false,
     "grade_id": "cell-0235e816fc98b0f6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<img src=\"https://cdn.pixabay.com/photo/2015/05/20/10/03/cat-and-dog-775116_960_720.jpg\" alt=\"Image of cats and dogs\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fdc628c18fc9d6f74c31d438a71ee482",
     "grade": false,
     "grade_id": "cell-c4bb694612153106",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "For this home assignment, we'll use the Kaggle dataset for the [Dogs vs. Cats competition](https://www.kaggle.com/c/dogs-vs-cats). It is comprised of 25k colour images of dogs and cats. Our goal with this dataset will be to create a classifier that can tell us if the input image is of a cat or a dog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "74fa6126f847f4dcd7beb20fd3883a15",
     "grade": false,
     "grade_id": "cell-ee9e2aee031325a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Using your cloud GPU\n",
    "As a way of helping you speed up the training process, each group gets access to a cloud instance with a GPU. Take a look at the [instructions folder](https://github.com/JulianoLagana/deep-machine-learning/blob/master/instructions/) to understand how to connect to an instance and use our tools there. You're free to use this limited resource as you see fit, but if you spend all your credits, you'll need a late day to obtain more (and you can only do this once).\n",
    "\n",
    "### Strong recommendation:\n",
    "In order to make the most out of your GPU hours, first try solving the initial part of this notebook (tasks 0-3) in your own computer (these tasks can be solved on the CPU), and leave most of the available hours for solving tasks 4-5, and refining your best model further (and, if you have the spare hours, experiment a bit!).\n",
    "\n",
    "### Working efficiently:\n",
    "Training for several epochs just to have your code break at the last validation step is incredibly frustrating and inefficient. Good practice is to first test long training runs with a much simpler dry-run: a single epoch, a few batches et c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "58e21e6f2c6b85e2c1f235d31baaf966",
     "grade": false,
     "grade_id": "cell-f7371c24b57c153e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Requirements:\n",
    "- Whenever we ask you to plot anything, be sure to add a title and label the axes. If you're plotting more than one curve in the same plot, also add a legend.\n",
    "- When we ask you to train an architecture, train it for a reasonable number of epochs. \"Reasonable\" here means you should be fairly confident that training for a higher number of epochs wouldn't impact your conclusions regarding the model's performance. When experimenting, a single epoch is often enough to tell whether your model setup has improved or not.\n",
    "\n",
    "\n",
    "Hints:\n",
    "- If you get errors saying you've exhausted the GPU resources, well, then you've exhausted the GPU resources. However, sometimes that's because `pytorch` didn't release a part of the GPU's memory. If you think your CNN should fit in your memory during training, try restarting the kernel and directly training only that architecture.\n",
    "- Every group has enough cloud credits to complete this assignment. However, this statement assumes you'll use your resources judiciously (e.g. always try the code first in your machine and make sure everything works properly before starting your instances) and **won't forget to stop your instance after using it,**  otherwise you might run out of credits.\n",
    "- Before starting, take a look at the images we'll be using. This is a hard task, don't get discouraged if your first models perform poorly (several participants in the original competition didn't achieve an accuracy higher than 60%).\n",
    "- Solving the [computer labs](https://github.com/JulianoLagana/deep-machine-learning/tree/master/computer-labs) is a good way to get prepared for this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f50e27a30d83bcebeb52a8ae43228e2",
     "grade": false,
     "grade_id": "cell-3ee6d24346a80d85",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 0. Imports\n",
    "\n",
    "In the following cell, add all the imports you'll use in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0075c816ac7a24f2287d6fa9b8a81565",
     "grade": true,
     "grade_id": "cell-464a08ede00083a4",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "import os\n",
    "import torch\n",
    "from argparse import ArgumentParser\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "#torch.cuda.set_device(0)\n",
    "#CUDA_VISIBLE_DEVICES=1\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from  torch.utils.data import random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "#torch.cuda.set_device(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device(0))\n",
    "# parser = ArgumentParser(description=‘Example’)\n",
    "# parser.add_argument(’–gpu’, type=int, default=[0,1], nargs=’+’, help=‘used gpu’)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# os.environ[“CUDA_VISIBLE_DEVICES”] = ‘,’.join(str(x) for x in args.gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "49bf801d5ced99ccf6f0c5cd12100230",
     "grade": false,
     "grade_id": "cell-4821dc273028d702",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 1. Loading the data and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d2cfdb936c364826fa8bc7a7adaf411c",
     "grade": false,
     "grade_id": "cell-2ea049dea4713494",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "The first step is to head to the [Kaggle website for the cats and dogs competition](https://www.kaggle.com/c/dogs-vs-cats/data) and download the data from there. You should download both the test and train folders together in one zip file (there is a `Download all` button at the bottom of the page). Unfortunately, you need to create a Kaggle account for this.\n",
    "\n",
    "**Only necessary for tasks 4-6**: Downloading the data to your local computer is quite straight-forward. Sooner or later you will have to upload the data to the cloud instance and that is a bit more tricky. There are a few ways to do it:\n",
    "\n",
    " - Jupyter Notebook upload function. When starting the notebook server with the command `jupyter notebook` you are directed to a main page. In the top right corner there is an upload button.\n",
    " - Using [`scp`](https://linuxize.com/post/how-to-use-scp-command-to-securely-transfer-files/) to copy files via an ssh connection.\n",
    " - Using the [Kaggle CLI](https://github.com/Kaggle/kaggle-api). We have added it to the conda environment.\n",
    "\n",
    "For this assignment we will again need data loaders. Like before we need to create a `Dataset` to give as input to a `DataLoader`. \n",
    "Fortunately, this type of image data is quite common so we get some help from `pytorch`. We can use [`ImageFolder`](https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder) to create a `Dataset` for our images. As long as our folder structure for the data conforms to the folder structure expected by `ImageFolder`, we can use it right out of the box and the `DataLoader` class will happily accept it as input.\n",
    "\n",
    "To use `ImageFolder` you should create a folder structure that resembles the following (obviously, the folder names are up to you):\n",
    "\n",
    "\n",
    "         small_train             small_val                train                   val\n",
    "              |                      |                      |                      |\n",
    "              |                      |                      |                      |\n",
    "        -------------          -------------          -------------          -------------\n",
    "        |           |          |           |          |           |          |           |\n",
    "        |           |          |           |          |           |          |           |\n",
    "      cats        dogs       cats        dogs       cats        dogs       cats        dogs\n",
    "\n",
    "\n",
    "The `small_train` and `small_val` folders have the training and validation samples for your smaller subset of the data, while the `train` and `val` folders contain all the samples you extracted from Kaggle's `train.zip`.\n",
    "This is just a convenient way of having a smaller dataset to play with for faster prototyping.\n",
    "\n",
    "We provide you a notebook that shows how to achieve this folder structure (`create_project_notebook_structure.ipynb`), starting from the original `dogs-vs-cats.zip` file that you download from Kaggle. If you do use that notebook, we encourage you to understand how each step is being done, so you can generalize this knowledge to new datasets you'll encounter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "687bc3927319df0106d64054f8854128",
     "grade": false,
     "grade_id": "cell-89ba19509b952af2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "For the smaller dataset, we advise you to use 70% of the data as training data (and thereby the remaining 30% for validation data). However, for the larger dataset, you should decide how to split between training and validation.\n",
    "\n",
    "**What percentage of the larger dataset did you decide to use for training?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70ba7076cdc41aac550685544ccb0fa2",
     "grade": true,
     "grade_id": "cell-7f3b0dfbd90a14c1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** Our original train dataset (train_all) have 25000 samples. We would like to choose 6% for validation set and 94% for train set. So we would have 1500 samples including 750 dogs and 750 cats for validation set (6% out of 25000 samples) and 23500 samples including 11750 dogs and 11750 for cats (94% out of 25000 samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a15cc209809b8aaaba643525ad54273",
     "grade": false,
     "grade_id": "cell-8964386e29e42eee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Optional (1 POE):** Did you decide to keep the same ratio split between train and validation sets for the larger dataset? Motivate your decision!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3748ce886332354dee82a16ffd9a9a3",
     "grade": true,
     "grade_id": "cell-88d41e16176067ca",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** No, We would like to choose 6% for validation set and 94% for train set. Our larger dataset has 25000 samples for training and 12500 samples for test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "834d9263037e9d6133fb6ca7faef761b",
     "grade": false,
     "grade_id": "cell-876ca7df88c9311f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Fill in the dataset paths (to be used later by your data loaders):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "977d3723abd484be4bfa4ffb4590a2d1",
     "grade": true,
     "grade_id": "cell-1b1314f2ab1b1d6b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Change the directories accordingly\n",
    "# YOUR CODE HERE\n",
    "train_path = \"/train\"\n",
    "val_path = \"/val\"\n",
    "small_train_path = \"/small_train\"\n",
    "small_val_path = \"/small_val\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6da7e41128b82828313028806f0ff72c",
     "grade": false,
     "grade_id": "cell-1d6ea64bca94a4ef",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "### 1.1 Preprocessing\n",
    "**(1 POE)** \n",
    "\n",
    "Once you have the expected folder structure, create two data loaders for automatically generating batches from the images in your smaller subset of data. It is here we choose how to preprocess the input data. There are multiple reasons for why we preprocess data:\n",
    "\n",
    "- Some transformations might be needed to actually make the data work with our network (reshaping, permuting dimensions et c.).\n",
    "- Make the training more efficient by making the input dimensions smaller, e.g. resizing, cropping.\n",
    "- Artificially expanding the training data through [data augmentation](https://cartesianfaith.com/2016/10/06/what-you-need-to-know-about-data-augmentation-for-machine-learning/)\n",
    "- We have some clever idea of how to change the data to make the training process better.\n",
    "\n",
    "We do not expect you to do data augmentation, but feel free to preprocess the data as you see fit.\n",
    "Construct an `ImageFolder` dataset like this:\n",
    "\n",
    "```python\n",
    "ImageFolder(<path_to_data_folder>, transform=Compose(<list_of_transforms>))\n",
    "# example:\n",
    "ImageFolder(Path.cwd() / \"small_train\", transform=Compose([ToTensor]))\n",
    "```\n",
    "\n",
    "Hints:\n",
    "- Take a look at [`ImageFolder`](https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder) and [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) from the pytorch docs.\n",
    "- To preprocess the data you can use the built-in pytorch [`Transforms`](https://pytorch.org/docs/stable/torchvision/transforms.html)\n",
    "- The `ImageFolder` dataset provides the data as a python image type. For easy conversion to a `torch.Tensor`, use the [`ToTensor`](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.ToTensor) transformation.\n",
    "- The specified `batch_size` should be chosen so that you train fast but don't run out of memory. You need to figure this out empirically; start small and increase the batch size until you run out of memory.\n",
    "- The `DataLoader` constructor takes an optional argument `num_workers`, which defaults to `0` if not provided. Setting a higher number creates multiple threads which load batches concurrently. This can speed up training considerably.  \n",
    "- When feeding the images to your CNN, you'll probably want all of them to have the same spatial size, even though the .jpeg files differ in this. Resizing the images can be done using the previously mentioned built-in pytorch Transforms.\n",
    "- Resizing the images to a smaller size while loading them can be beneficial. The VGG network that is used later in this assignment requires that images are at least 224x224, but before that use small images to speed up training. The CNN's do surprisingly well on 64x64 or even 32x32 images. Shorter training cycles give your more time to experiment!\n",
    "\n",
    "We encourage you to explore the data and choose transformations that you believe to be useful. For exploration we provide you with some helper functions to visually compare transformations side by side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b63cfb45608fab1b1ddda42ffb5aa32",
     "grade": false,
     "grade_id": "cell-5ca8fc808d4ee65b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compare_transforms(transformations, index):\n",
    "    \"\"\"Visually compare transformations side by side.\n",
    "    Takes a list of ImageFolder datasets with different compositions of transformations.\n",
    "    It then display the `index`th image of the dataset for each transformed dataset in the list.\n",
    "    \n",
    "    Example usage:\n",
    "        compare_transforms([dataset_with_transform_1, dataset_with_transform_2], 0)\n",
    "    \n",
    "    Args:\n",
    "        transformations (list(ImageFolder)): list of ImageFolder instances with different transformations\n",
    "        index (int): Index of the sample in the ImageFolder you wish to compare.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Here we combine two neat functions from basic python to validate the input to the function:\n",
    "    # - `all` takes an iterable (something we can loop over, like a list) of booleans\n",
    "    #    and returns True if every element is True, otherwise it returns False.\n",
    "    # - `isinstance` checks whether a variable is an instance of a particular type (class)\n",
    "    if not all(isinstance(transf, ImageFolder) for transf in transformations):\n",
    "        raise TypeError(\"All elements in the `transformations` list need to be of type ImageFolder\")\n",
    "        \n",
    "    num_transformations = len(transformations)\n",
    "    fig, axes = plt.subplots(1, num_transformations)\n",
    "    \n",
    "    # This is just a hack to make sure that `axes` is a list of the same length as `transformations`.\n",
    "    # If we only have one element in the list, `plt.subplots` will not create a list of a single axis\n",
    "    # but rather just an axis without a list.\n",
    "    if num_transformations == 1:\n",
    "        axes = [axes]\n",
    "        \n",
    "    for counter, (axis, transf) in enumerate(zip(axes, transformations)):\n",
    "        axis.set_title(\"transf: {}\".format(counter))\n",
    "        image_tensor = transf[index][0]\n",
    "        display_image(axis, image_tensor)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def display_image(axis, image_tensor):\n",
    "    \"\"\"Display a tensor as image\n",
    "    \n",
    "    Example usage:\n",
    "        _, axis = plt.subplots()\n",
    "        some_random_index = 453\n",
    "        image_tensor, _ = train_dataset[some_random_index]\n",
    "        display_image(axis, image_tensor)\n",
    "    \n",
    "    Args:\n",
    "        axis (pyplot axis)\n",
    "        image_tensor (torch.Tensor): tensor with shape (num_channels=3, width, heigth)\n",
    "    \"\"\"\n",
    "    \n",
    "    # See hint above\n",
    "    if not isinstance(image_tensor, torch.Tensor):\n",
    "        raise TypeError(\"The `display_image` function expects a `torch.Tensor` \" +\n",
    "                        \"use the `ToTensor` transformation to convert the images to tensors.\")\n",
    "        \n",
    "    # The imshow commands expects a `numpy array` with shape (3, width, height)\n",
    "    # We rearrange the dimensions with `permute` and then convert it to `numpy`\n",
    "    image_data = image_tensor.permute(1, 2, 0).numpy()\n",
    "    height, width, _ = image_data.shape\n",
    "    axis.imshow(image_data)\n",
    "    axis.set_xlim(0, width)\n",
    "    # By convention when working with images, the origin is at the top left corner.\n",
    "    # Therefore, we switch the order of the y limits.\n",
    "    axis.set_ylim(height, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([3500, 3, 64, 64])\n",
      "[0.48770484 0.45318767 0.41516972] [0.252033   0.24500722 0.24762712]\n",
      "0\n",
      "torch.Size([1500, 3, 64, 64])\n",
      "[0.49427933 0.45639446 0.4175132 ] [0.25301176 0.24703856 0.2492882 ]\n"
     ]
    }
   ],
   "source": [
    "TRANSFORM_IMG = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.CenterCrop(64),\n",
    "    \n",
    "    #transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "  #  transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "  #                       std=[0.229, 0.224, 0.225] )\n",
    "    ])\n",
    "\n",
    "# TRANSFORM_IMG = transforms.Compose([\n",
    "#     transforms.Resize(68),\n",
    "#     transforms.CenterCrop(68),\n",
    "    \n",
    "#     #transforms.ToPILImage(),\n",
    "#     transforms.ToTensor(),\n",
    "#   #  transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#   #                       std=[0.229, 0.224, 0.225] )\n",
    "#     ])\n",
    "\n",
    "\n",
    "\n",
    "train_dataset= ImageFolder( root=\"small_train\",transform=TRANSFORM_IMG )\n",
    "dataloader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=False, num_workers=4)\n",
    "pop_mean = []\n",
    "pop_std0 = []\n",
    "pop_std1 = []\n",
    "for i, data in enumerate(dataloader, 0):\n",
    "    # shape (batch_size, 3, height, width)\n",
    "    print(i)\n",
    "    print(data[0].shape)\n",
    "    numpy_image = data[0].numpy()\n",
    "    \n",
    "    # shape (3,)\n",
    "    small_train_mean = np.mean(numpy_image, axis=(0,2,3))\n",
    "    small_train_std = np.std(numpy_image, axis=(0,2,3))\n",
    "  #  batch_std1 = np.std(numpy_image, axis=(0,2,3), ddof=1)\n",
    "    print(small_train_mean,small_train_std)\n",
    "#     pop_mean.append(batch_mean)\n",
    "#     pop_std0.append(batch_std0)\n",
    "#     pop_std1.append(batch_std1)\n",
    "\n",
    "#print(pop_mean)\n",
    "#print(pop_std0)\n",
    "#print(pop_std1)\n",
    "\n",
    "##################################################################################################\n",
    "val_dataset= ImageFolder( root=\"small_val\",transform=TRANSFORM_IMG )\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False, num_workers=4)\n",
    "# pop_mean = []\n",
    "# pop_std0 = []\n",
    "# pop_std1 = []\n",
    "for i, data in enumerate(val_dataloader, 0):\n",
    "    # shape (batch_size, 3, height, width)\n",
    "    print(i)\n",
    "    print(data[0].shape)\n",
    "    numpy_image = data[0].numpy()\n",
    "    \n",
    "    # shape (3,)\n",
    "    small_val_mean = np.mean(numpy_image, axis=(0,2,3))\n",
    "    small_val_std = np.std(numpy_image, axis=(0,2,3))\n",
    "  #  batch_std1 = np.std(numpy_image, axis=(0,2,3), ddof=1)\n",
    "    print(small_val_mean,small_val_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "493b16ca331bf9f2d42ed571caec3130",
     "grade": true,
     "grade_id": "cell-31b81f052b6e681e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                                  else \"cpu\")\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "     #                    std=[0.229, 0.224, 0.225] )\n",
    "print(device)\n",
    "\n",
    "TRANSFORM_TRAINIMG = transforms.Compose([\n",
    "    transforms.Resize(64),    # 96 64 112  224 68   32  \n",
    "    transforms.CenterCrop(64),  # 96 64 112 224 68   32\n",
    "    \n",
    "    #transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=small_train_mean,\n",
    "                         std =small_train_std )\n",
    "    ])\n",
    "TRANSFORM_VALIMG = transforms.Compose([\n",
    "    transforms.Resize(64),  #224 68   32\n",
    "    transforms.CenterCrop(64),   #224 68   32\n",
    "    \n",
    "    #transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=small_val_std,\n",
    "                         std =small_val_std )\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "# TRANSFORM_IMG2 = transforms.Compose([\n",
    "#     transforms.Resize(64),\n",
    "#     transforms.CenterCrop(64),\n",
    "#     transforms.ToTensor(),\n",
    "#   #  transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#   #                       std=[0.229, 0.224, 0.225])\n",
    "#     ])\n",
    "\n",
    "\n",
    "# train_dataset2= ImageFolder( root=\"small_train\", transform=transforms.Compose([transforms.ToTensor()]))\n",
    "# loader = DataLoader(train_dataset2, batch_size=len(train_dataset2), num_workers=1)\n",
    "# data = next(iter(loader))\n",
    "# print(data[0].mean(), data[0].std())\n",
    "\n",
    "# train_dataset= ImageFolder( root=\"small_train\",transform=TRANSFORM_TRAINIMG )\n",
    "# dataloader = DataLoader(train_dataset, batch_size=3500, shuffle=False, num_workers=4)\n",
    "# pop_mean = []\n",
    "# pop_std0 = []\n",
    "# pop_std1 = []\n",
    "# for i, data in enumerate(dataloader, 0):\n",
    "    # shape (batch_size, 3, height, width)\n",
    "#     print(i)\n",
    "#     print(type(data))\n",
    "#     print(len(data))\n",
    "#     arr=np.array(data[0])\n",
    "#     print(arr.shape)\n",
    "#     print(data[0].shape)\n",
    "    #print(data.size)\n",
    "#     numpy_image = data[0].numpy()\n",
    "    \n",
    "#     # shape (3,)\n",
    "#     batch_mean = np.mean(numpy_image, axis=(0,2,3))\n",
    "#     batch_std0 = np.std(numpy_image, axis=(0,2,3))\n",
    "#     batch_std1 = np.std(numpy_image, axis=(0,2,3), ddof=1)\n",
    "    \n",
    "#     pop_mean.append(batch_mean)\n",
    "#     pop_std0.append(batch_std0)\n",
    "#     pop_std1.append(batch_std1)\n",
    "\n",
    "# print(pop_mean)\n",
    "# print(pop_std0)\n",
    "# print(pop_std1)\n",
    "# shape (num_iterations, 3) -> (mean across 0th axis) -> shape (3,)\n",
    "#pop_mean = np.array(pop_mean).mean(axis=0)\n",
    "#pop_std0 = np.array(pop_std0).mean(axis=0)\n",
    "#pop_std1 = np.array(pop_std1).mean(axis=0)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset= ImageFolder( root=\"small_train\", transform=TRANSFORM_TRAINIMG )\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=False, num_workers=4)\n",
    "\n",
    "val_dataset= ImageFolder( root=\"small_val\", transform=TRANSFORM_VALIMG ) \n",
    "\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=256, num_workers=4)\n",
    "\n",
    "#print(type(dataset_1))\n",
    "#print(type(data_loader_1))\n",
    "\n",
    "\n",
    "compare_transforms([train_dataset,train_dataset], 2500)\n",
    "j=0\n",
    "for i, (data, labels) in enumerate(train_data_loader):\n",
    "    j= j+1\n",
    "print(j)    \n",
    "   # print(type(data))\n",
    "   # print(data.shape)\n",
    "  #  print(labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "16c576d5cad5ec453068ac139dc3f454",
     "grade": false,
     "grade_id": "cell-24463974ae20a276",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(2 POE)** How did you select transformations, if any? Briefly explain your reasoning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "efec72804bea1b6cec8eba964299148a",
     "grade": true,
     "grade_id": "cell-051ee24a83af3cf8",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** \n",
    "1- Resize (transforms.Resize) \n",
    "2- CenterCrop (transforms.CenterCrop)\n",
    "3- Normalize (transforms.Normalize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f7d18f73a495811569bd83851d125b4",
     "grade": false,
     "grade_id": "cell-c0bfc1ac7fadfcc7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 2. Training\n",
    "\n",
    "**(1 POE)**\n",
    "\n",
    "Create your first CNN architecture for this task. Start with something as simple as possible, that you're almost sure can get an accuracy better than 50% (we'll improve upon it later).\n",
    "Naturally, you must also select a loss function and an optimizer.\n",
    "\n",
    "Hints:\n",
    "\n",
    "- Training on a CPU is slow and in the beginning you just want to verify that your architecture actually produces a predicition with the correct shape. Make everything you can to speed up the prototyping phase, e.g. train only for a single epoch and make the images ridiculously small.\n",
    "- Going from the last CNN layer to the final fully connected layer is not trivial. The convolutions produces \"3D\" output which we can think of as an image with many channels, while the fully connected layer expects a row vector as input. Calculate how many output features the convolutions produce and use `.reshape` to make your tensor fit the fully connected layer. (It is also common to see the `.view` method to do the same thing. They basically do the same thing but have some differences in internal memory management.) *Hint within the hint:* remember that the fully connected layers expects a *batch* of 1D tensors. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2640d61f483bd3ac9c018fd407f71ace",
     "grade": true,
     "grade_id": "cell-4c9de348cd8bc4ff",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "class ImageClassifierNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=20, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=20, out_channels=50, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(4*4*50, 150)  #4x4x50\n",
    "        self.fc2 = nn.Linear(150, 2)\n",
    "       # self.fc3 = nn.Linear(500, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = x.view(-1, 4*4*50)   #4x4x50\n",
    "        x = F.relu(self.fc1(x))\n",
    "      #  x = F.relu(self.fc2(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "        #return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8bcf22ee899c7b8a527f4d0f7e2a962d",
     "grade": false,
     "grade_id": "cell-cb6fc78116ad6b75",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Train your model using the two data loaders you created earlier. Train for a reasonable amount of epochs, so as to get a good sense of how well this architecture performs.\n",
    "\n",
    "Hints:\n",
    "- Note that you will need to plot your training and validation losses and accuracies, so make sure that you saved them during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "510f9590642bc2210d710bfc6fef2b97",
     "grade": true,
     "grade_id": "cell-bb1fcd878f3bea9a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Any pytorch object (e.g. model, inputs, output, etc.) can \n",
    "# be transferred to the current device by running\n",
    "#       name_of_object.to(device)\n",
    "# Example:\n",
    "#       model.to(device)\n",
    "#\n",
    "# The following line automatically figures out what device (cpu or gpu)\n",
    "# you are using and stores the result in `device`.\n",
    "# Later we can use the `.to(device)` method to move our data or model to the correct device.\n",
    "\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "model = ImageClassifierNetwork()\n",
    "model.to(device);\n",
    "#loss_fn = nn.NLLLoss()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.0001,momentum=0.5)\n",
    "\n",
    "\n",
    "def evaluate_model(val_data_loader, model, loss_fn):\n",
    "    losses = []\n",
    "    n_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for b_x, b_y in val_data_loader:\n",
    "            b_x=b_x.to(device)\n",
    "            b_y=b_y.to(device)\n",
    "            pred = model(b_x)\n",
    "            loss = loss_fn(pred, b_y)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            hard_preds = pred.argmax(dim=1)\n",
    "            n_correct += torch.sum(pred.argmax(dim=1) == b_y).item()\n",
    "        val_accuracy = n_correct/len(val_dataset)\n",
    "        val_avg_loss = sum(losses)/len(losses)    \n",
    "    \n",
    "    return val_accuracy, val_avg_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \tLoss: 0.696 \tLoss (val): 0.689\tAccuracy: 0.50 \tAccuracy (val): 0.50\n",
      "Epoch 1 \tLoss: 0.674 \tLoss (val): 0.667\tAccuracy: 0.58 \tAccuracy (val): 0.61\n",
      "Epoch 2 \tLoss: 0.643 \tLoss (val): 0.719\tAccuracy: 0.63 \tAccuracy (val): 0.55\n",
      "Epoch 3 \tLoss: 0.617 \tLoss (val): 0.658\tAccuracy: 0.66 \tAccuracy (val): 0.62\n",
      "Epoch 4 \tLoss: 0.608 \tLoss (val): 0.606\tAccuracy: 0.67 \tAccuracy (val): 0.66\n",
      "Epoch 5 \tLoss: 0.563 \tLoss (val): 0.653\tAccuracy: 0.71 \tAccuracy (val): 0.65\n",
      "Epoch 6 \tLoss: 0.554 \tLoss (val): 0.598\tAccuracy: 0.72 \tAccuracy (val): 0.66\n",
      "Epoch 7 \tLoss: 0.541 \tLoss (val): 0.587\tAccuracy: 0.72 \tAccuracy (val): 0.68\n",
      "Epoch 8 \tLoss: 0.501 \tLoss (val): 0.622\tAccuracy: 0.74 \tAccuracy (val): 0.67\n",
      "Epoch 9 \tLoss: 0.462 \tLoss (val): 0.553\tAccuracy: 0.78 \tAccuracy (val): 0.72\n",
      "Epoch 10 \tLoss: 0.427 \tLoss (val): 0.583\tAccuracy: 0.80 \tAccuracy (val): 0.73\n",
      "Epoch 11 \tLoss: 0.391 \tLoss (val): 0.616\tAccuracy: 0.82 \tAccuracy (val): 0.72\n",
      "Epoch 12 \tLoss: 0.342 \tLoss (val): 0.628\tAccuracy: 0.85 \tAccuracy (val): 0.72\n",
      "Epoch 13 \tLoss: 0.305 \tLoss (val): 0.565\tAccuracy: 0.87 \tAccuracy (val): 0.74\n",
      "Epoch 14 \tLoss: 0.271 \tLoss (val): 0.670\tAccuracy: 0.89 \tAccuracy (val): 0.73\n",
      "Epoch 15 \tLoss: 0.236 \tLoss (val): 0.677\tAccuracy: 0.91 \tAccuracy (val): 0.74\n",
      "Epoch 16 \tLoss: 0.191 \tLoss (val): 0.719\tAccuracy: 0.93 \tAccuracy (val): 0.77\n",
      "Epoch 17 \tLoss: 0.145 \tLoss (val): 0.809\tAccuracy: 0.95 \tAccuracy (val): 0.74\n",
      "Epoch 18 \tLoss: 0.084 \tLoss (val): 1.535\tAccuracy: 0.97 \tAccuracy (val): 0.68\n",
      "Epoch 19 \tLoss: 0.081 \tLoss (val): 1.149\tAccuracy: 0.97 \tAccuracy (val): 0.75\n",
      "Epoch 20 \tLoss: 0.068 \tLoss (val): 1.012\tAccuracy: 0.98 \tAccuracy (val): 0.76\n",
      "Epoch 21 \tLoss: 0.038 \tLoss (val): 1.166\tAccuracy: 0.99 \tAccuracy (val): 0.76\n",
      "Epoch 22 \tLoss: 0.025 \tLoss (val): 1.401\tAccuracy: 0.99 \tAccuracy (val): 0.75\n",
      "Epoch 23 \tLoss: 0.039 \tLoss (val): 1.257\tAccuracy: 0.99 \tAccuracy (val): 0.75\n",
      "Epoch 24 \tLoss: 0.060 \tLoss (val): 1.316\tAccuracy: 0.98 \tAccuracy (val): 0.72\n",
      "Epoch 25 \tLoss: 0.033 \tLoss (val): 1.375\tAccuracy: 0.99 \tAccuracy (val): 0.73\n",
      "Epoch 26 \tLoss: 0.013 \tLoss (val): 1.404\tAccuracy: 1.00 \tAccuracy (val): 0.75\n",
      "Epoch 27 \tLoss: 0.010 \tLoss (val): 1.795\tAccuracy: 1.00 \tAccuracy (val): 0.71\n",
      "Epoch 28 \tLoss: 0.008 \tLoss (val): 1.757\tAccuracy: 1.00 \tAccuracy (val): 0.74\n",
      "Epoch 29 \tLoss: 0.002 \tLoss (val): 1.884\tAccuracy: 1.00 \tAccuracy (val): 0.74\n",
      "Epoch 30 \tLoss: 0.001 \tLoss (val): 1.775\tAccuracy: 1.00 \tAccuracy (val): 0.75\n",
      "Epoch 31 \tLoss: 0.000 \tLoss (val): 1.902\tAccuracy: 1.00 \tAccuracy (val): 0.75\n",
      "Epoch 32 \tLoss: 0.000 \tLoss (val): 2.054\tAccuracy: 1.00 \tAccuracy (val): 0.74\n",
      "Epoch 33 \tLoss: 0.000 \tLoss (val): 2.086\tAccuracy: 1.00 \tAccuracy (val): 0.74\n",
      "Epoch 34 \tLoss: 0.000 \tLoss (val): 2.247\tAccuracy: 1.00 \tAccuracy (val): 0.74\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(35):\n",
    "    losses = []\n",
    "    n_correct = 0\n",
    "    for b_x, b_y in train_data_loader:\n",
    "        b_x=b_x.to(device)\n",
    "        b_y=b_y.to(device)\n",
    "        \n",
    "        # Compute predictions and losses\n",
    "        pred = model(b_x)\n",
    "        loss = loss_fn(pred, b_y)\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Count number of correct predictions\n",
    "        hard_preds = pred.argmax(dim=1)\n",
    "        n_correct += torch.sum(pred.argmax(dim=1) == b_y).item()\n",
    "\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()    \n",
    "        \n",
    "    # Compute accuracy and loss in the entire training set\n",
    "    train_accuracy = n_correct/len(train_dataset)\n",
    "    train_avg_loss = sum(losses)/len(losses)    \n",
    "    \n",
    "\n",
    "    # Compute accuracy and loss in the entire validation set\n",
    "   \n",
    "    val_accuracy, val_avg_loss = evaluate_model(val_data_loader, model, loss_fn)\n",
    "  \n",
    "    \n",
    "    \n",
    "    # Display metrics\n",
    "    display_str = 'Epoch {} '\n",
    "    display_str += '\\tLoss: {:.3f} '\n",
    "    display_str += '\\tLoss (val): {:.3f}'\n",
    "    display_str += '\\tAccuracy: {:.2f} '\n",
    "    display_str += '\\tAccuracy (val): {:.2f}'\n",
    "    print(display_str.format(epoch, train_avg_loss, val_avg_loss, train_accuracy, val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da9811c873c0b5e3672930290854c501",
     "grade": false,
     "grade_id": "cell-4d42c86687697a67",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Create two plots. In one of them, plot the loss in the training and the validation datasets. In the other one, plot the accuracy in the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e71729e832321272983178b41b90f874",
     "grade": true,
     "grade_id": "cell-fa81712e1e27432a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "%matplotlib notebook\n",
    "\n",
    "# Create figure for plotting\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(8,4))\n",
    "plt.ion()\n",
    "plot_interval = 1\n",
    "\n",
    "\n",
    "# Create arrays to save all of the metrics throughout training\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(2):\n",
    "    for i, (b_x, b_y) in enumerate(train_data_loader):\n",
    "        # Compute predictions and loss\n",
    "        b_x=b_x.to(device)\n",
    "        b_y=b_y.to(device)\n",
    "        pred = model(b_x)\n",
    "        loss = loss_fn(pred, b_y)\n",
    "    \n",
    "        # Back-propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute metrics\n",
    "        hard_preds = pred.argmax(dim=1)\n",
    "        n_correct = torch.sum(pred.argmax(dim=1) == b_y).item()\n",
    "        val_accuracy, val_avg_loss = evaluate_model(val_data_loader, model, loss_fn)\n",
    "        \n",
    "        # Save them in the arrays\n",
    "        train_losses.append(loss.item())\n",
    "        val_losses.append(val_avg_loss)\n",
    "        train_accs.append(n_correct/len(b_x))\n",
    "        val_accs.append(val_accuracy)\n",
    "        \n",
    "        if i % plot_interval == 0:\n",
    "            # Update plots\n",
    "            ax[0].clear()\n",
    "            ax[0].plot(train_losses)\n",
    "            ax[0].plot(val_losses)\n",
    "\n",
    "            ax[1].clear()\n",
    "            ax[1].plot(train_accs)\n",
    "            ax[1].plot(val_accs)\n",
    "\n",
    "            # Add legends and labels\n",
    "            ax[0].set_title('Loss')\n",
    "            ax[0].set_xlabel('Number of batches')\n",
    "            ax[0].legend(['Train', 'Validation'])\n",
    "\n",
    "            ax[1].set_title('Accuracy')\n",
    "            ax[1].set_xlabel('Number of batches')\n",
    "            ax[1].legend(['Train', 'Validation'])\n",
    "            ax[1].set_ylim([0,1])\n",
    "\n",
    "            # Draw the figure on the screen\n",
    "            fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7acc519e5a759ed1c886e05ff54eced1",
     "grade": false,
     "grade_id": "cell-f2fc166890962bcf",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(2 POE)** Based on these, what would you suggest for improving your model? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2b42c08ae7bcb61726d89b8dd46922d6",
     "grade": true,
     "grade_id": "cell-506e21ce469b67f5",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** \n",
    "1- Adding more convolutional layer\n",
    "2- using smaller filters   deeper, more non-linearities and fewer parameters \n",
    "3- increase size of input image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "afa8e2c066a79d25a5ad58e8095cbfac",
     "grade": false,
     "grade_id": "cell-ee79a83a62b70a8f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 3. Improving your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b093333dbbf35daf4261a0acdaec5a4",
     "grade": false,
     "grade_id": "cell-5314d286e79e0377",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(1 POE)** Continue to improve your model architecture by comparing the value of the metrics you're interested in for both the training and validation set. Try different ideas! When you're happy with one architecture, copy it in the cell below and train it here. Save the training and validation losses and accuracies. You'll use this later to compare your best model with the one using transfer learning.\n",
    "\n",
    "**Note**: When trying different ideas, you'll end up with several different models. However, when submitting your solutions to Canvas, the cell below must contain only the definition and training of *one* model. Remove all code related to the models that were not chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input 96x96x3\n",
    "class ImageClassifierNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3,padding=1)\n",
    "        #\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,padding=1)\n",
    "        #\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3,padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "       # self.conv7 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,padding=1)\n",
    "        \n",
    "        \n",
    "      #  self.conv8 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "     #   self.conv9 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "#         self.conv10 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3,padding=1)\n",
    "        \n",
    "#         self.conv11 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "     #   self.conv10_drop = nn.Dropout2d()\n",
    "    \n",
    "        self.fc1 = nn.Linear(12*12*256, 2)\n",
    "    #    self.fc2 = nn.Linear(150, 2)\n",
    "#         self.fc3 = nn.Linear(150, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #48  #56   #32\n",
    "              \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #24 #28   #16\n",
    "        \n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "      #  x = F.relu(self.conv7(x))     \n",
    "        x = F.max_pool2d(x, kernel_size=2) #12 #14   #8\n",
    "        \n",
    "#         x = F.relu(self.conv11(x)) \n",
    "#         x = F.max_pool2d(x, kernel_size=2) #14   #4\n",
    "        \n",
    "      #  x = F.relu(self.conv8(x))   \n",
    "     #   x = F.relu(self.conv9(x))   \n",
    "#         x = F.relu(self.conv10(x)) \n",
    "#         x = F.relu(self.conv10_drop(x))\n",
    "      #  x = F.max_pool2d(x, kernel_size=2) #6    #[14x14x512] if i=224x224    7x7x512 if i=112\n",
    "        \n",
    "   #     x = F.relu(self.conv11(x))\n",
    "   #     x = F.max_pool2d(x, kernel_size=2) #[7x7x512]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        x = x.view(-1, 12*12*256)\n",
    "      #  x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "       # x = F.dropout(x, training=self.training)\n",
    "        x = self.fc1(x)\n",
    "     #   return F.softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "# Epoch 0 \tLoss: 0.699 \tLoss (val): 0.693\tAccuracy: 0.51 \tAccuracy (val): 0.50\n",
    "# Epoch 1 \tLoss: 0.684 \tLoss (val): 0.672\tAccuracy: 0.55 \tAccuracy (val): 0.56\n",
    "# Epoch 2 \tLoss: 0.651 \tLoss (val): 0.652\tAccuracy: 0.62 \tAccuracy (val): 0.62\n",
    "# Epoch 3 \tLoss: 0.609 \tLoss (val): 0.641\tAccuracy: 0.67 \tAccuracy (val): 0.63\n",
    "# Epoch 4 \tLoss: 0.604 \tLoss (val): 0.638\tAccuracy: 0.67 \tAccuracy (val): 0.62\n",
    "# Epoch 5 \tLoss: 0.589 \tLoss (val): 0.617\tAccuracy: 0.69 \tAccuracy (val): 0.66\n",
    "# Epoch 6 \tLoss: 0.568 \tLoss (val): 0.639\tAccuracy: 0.71 \tAccuracy (val): 0.65\n",
    "# Epoch 7 \tLoss: 0.548 \tLoss (val): 0.691\tAccuracy: 0.73 \tAccuracy (val): 0.64\n",
    "# Epoch 8 \tLoss: 0.521 \tLoss (val): 0.623\tAccuracy: 0.74 \tAccuracy (val): 0.67\n",
    "# Epoch 9 \tLoss: 0.498 \tLoss (val): 0.592\tAccuracy: 0.76 \tAccuracy (val): 0.71\n",
    "# Epoch 10 \tLoss: 0.470 \tLoss (val): 0.577\tAccuracy: 0.77 \tAccuracy (val): 0.71\n",
    "# Epoch 11 \tLoss: 0.437 \tLoss (val): 0.578\tAccuracy: 0.79 \tAccuracy (val): 0.71\n",
    "# Epoch 12 \tLoss: 0.377 \tLoss (val): 0.665\tAccuracy: 0.83 \tAccuracy (val): 0.71\n",
    "# Epoch 13 \tLoss: 0.370 \tLoss (val): 0.640\tAccuracy: 0.83 \tAccuracy (val): 0.71\n",
    "# Epoch 14 \tLoss: 0.289 \tLoss (val): 0.631\tAccuracy: 0.88 \tAccuracy (val): 0.75\n",
    "# Epoch 15 \tLoss: 0.267 \tLoss (val): 0.723\tAccuracy: 0.89 \tAccuracy (val): 0.72\n",
    "# Epoch 16 \tLoss: 0.193 \tLoss (val): 0.767\tAccuracy: 0.92 \tAccuracy (val): 0.74\n",
    "# Epoch 17 \tLoss: 0.138 \tLoss (val): 0.934\tAccuracy: 0.95 \tAccuracy (val): 0.75\n",
    "# Epoch 18 \tLoss: 0.116 \tLoss (val): 0.964\tAccuracy: 0.96 \tAccuracy (val): 0.75\n",
    "# Epoch 19 \tLoss: 0.068 \tLoss (val): 1.231\tAccuracy: 0.98 \tAccuracy (val): 0.74\n",
    "# Epoch 20 \tLoss: 0.052 \tLoss (val): 1.523\tAccuracy: 0.98 \tAccuracy (val): 0.74\n",
    "# Epoch 21 \tLoss: 0.081 \tLoss (val): 1.382\tAccuracy: 0.97 \tAccuracy (val): 0.73\n",
    "# Epoch 22 \tLoss: 0.051 \tLoss (val): 1.633\tAccuracy: 0.98 \tAccuracy (val): 0.74\n",
    "# Epoch 23 \tLoss: 0.035 \tLoss (val): 1.534\tAccuracy: 0.99 \tAccuracy (val): 0.73\n",
    "# Epoch 24 \tLoss: 0.024 \tLoss (val): 1.875\tAccuracy: 0.99 \tAccuracy (val): 0.74\n",
    "# Epoch 25 \tLoss: 0.014 \tLoss (val): 1.821\tAccuracy: 1.00 \tAccuracy (val): 0.73\n",
    "# Epoch 26 \tLoss: 0.006 \tLoss (val): 2.076\tAccuracy: 1.00 \tAccuracy (val): 0.73\n",
    "# Epoch 27 \tLoss: 0.003 \tLoss (val): 2.032\tAccuracy: 1.00 \tAccuracy (val): 0.74\n",
    "# Epoch 28 \tLoss: 0.001 \tLoss (val): 2.245\tAccuracy: 1.00 \tAccuracy (val): 0.74\n",
    "# Epoch 29 \tLoss: 0.001 \tLoss (val): 2.193\tAccuracy: 1.00 \tAccuracy (val): 0.74\n",
    "# Epoch 30 \tLoss: 0.000 \tLoss (val): 2.252\tAccuracy: 1.00 \tAccuracy (val): 0.74\n",
    "# Epoch 31 \tLoss: 0.000 \tLoss (val): 2.297\tAccuracy: 1.00 \tAccuracy (val): 0.74\n",
    "# Epoch 32 \tLoss: 0.000 \tLoss (val): 2.337\tAccuracy: 1.00 \tAccuracy (val): 0.74\n",
    "# Epoch 33 \tLoss: 0.000 \tLoss (val): 2.370\tAccuracy: 1.00 \tAccuracy (val): 0.74\n",
    "# Epoch 34 \tLoss: 0.000 \tLoss (val): 2.401\tAccuracy: 1.00 \tAccuracy (val): 0.74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input 96x96x3\n",
    "class ImageClassifierNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3,padding=1)\n",
    "        #\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,padding=1)\n",
    "        #\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3,padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "       # self.conv7 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,padding=1)\n",
    "        \n",
    "        \n",
    "        self.conv8 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "        self.conv9 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "#         self.conv10 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3,padding=1)\n",
    "        \n",
    "#         self.conv11 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "     #   self.conv10_drop = nn.Dropout2d()\n",
    "    \n",
    "        self.fc1 = nn.Linear(6*6*256, 2)\n",
    "    #    self.fc2 = nn.Linear(150, 2)\n",
    "#         self.fc3 = nn.Linear(150, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #48  #56   #32\n",
    "              \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #24 #28   #16\n",
    "        \n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "      #  x = F.relu(self.conv7(x))     \n",
    "        x = F.max_pool2d(x, kernel_size=2) #12 #14   #8\n",
    "        \n",
    "#         x = F.relu(self.conv11(x)) \n",
    "#         x = F.max_pool2d(x, kernel_size=2) #14   #4\n",
    "        \n",
    "        x = F.relu(self.conv8(x))   \n",
    "        x = F.relu(self.conv9(x))   \n",
    "#         x = F.relu(self.conv10(x)) \n",
    "#         x = F.relu(self.conv10_drop(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #6    #[14x14x512] if i=224x224    7x7x512 if i=112\n",
    "        \n",
    "   #     x = F.relu(self.conv11(x))\n",
    "   #     x = F.max_pool2d(x, kernel_size=2) #[7x7x512]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        x = x.view(-1, 6*6*256)\n",
    "      #  x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "       # x = F.dropout(x, training=self.training)\n",
    "        x = self.fc1(x)\n",
    "     #   return F.softmax(x, dim=1)\n",
    "        return x\n",
    "# Epoch 0 \tLoss: 0.696 \tLoss (val): 0.693\tAccuracy: 0.50 \tAccuracy (val): 0.50\n",
    "# Epoch 1 \tLoss: 0.693 \tLoss (val): 0.691\tAccuracy: 0.51 \tAccuracy (val): 0.51\n",
    "# Epoch 2 \tLoss: 0.687 \tLoss (val): 0.693\tAccuracy: 0.56 \tAccuracy (val): 0.50\n",
    "# Epoch 3 \tLoss: 0.683 \tLoss (val): 0.712\tAccuracy: 0.54 \tAccuracy (val): 0.55\n",
    "# Epoch 4 \tLoss: 0.671 \tLoss (val): 0.677\tAccuracy: 0.60 \tAccuracy (val): 0.60\n",
    "# Epoch 5 \tLoss: 0.669 \tLoss (val): 0.715\tAccuracy: 0.60 \tAccuracy (val): 0.59\n",
    "# Epoch 6 \tLoss: 0.645 \tLoss (val): 0.705\tAccuracy: 0.63 \tAccuracy (val): 0.58\n",
    "# Epoch 7 \tLoss: 0.619 \tLoss (val): 0.676\tAccuracy: 0.66 \tAccuracy (val): 0.61\n",
    "# Epoch 8 \tLoss: 0.607 \tLoss (val): 0.622\tAccuracy: 0.67 \tAccuracy (val): 0.64\n",
    "# Epoch 9 \tLoss: 0.582 \tLoss (val): 0.659\tAccuracy: 0.70 \tAccuracy (val): 0.63\n",
    "# Epoch 10 \tLoss: 0.590 \tLoss (val): 0.632\tAccuracy: 0.69 \tAccuracy (val): 0.65\n",
    "# Epoch 11 \tLoss: 0.566 \tLoss (val): 0.623\tAccuracy: 0.72 \tAccuracy (val): 0.65\n",
    "# Epoch 12 \tLoss: 0.562 \tLoss (val): 0.613\tAccuracy: 0.72 \tAccuracy (val): 0.68\n",
    "# Epoch 13 \tLoss: 0.552 \tLoss (val): 0.708\tAccuracy: 0.73 \tAccuracy (val): 0.60\n",
    "# Epoch 14 \tLoss: 0.537 \tLoss (val): 0.578\tAccuracy: 0.74 \tAccuracy (val): 0.69\n",
    "# Epoch 15 \tLoss: 0.514 \tLoss (val): 0.588\tAccuracy: 0.74 \tAccuracy (val): 0.70\n",
    "# Epoch 16 \tLoss: 0.501 \tLoss (val): 0.588\tAccuracy: 0.76 \tAccuracy (val): 0.69\n",
    "# Epoch 17 \tLoss: 0.477 \tLoss (val): 0.607\tAccuracy: 0.78 \tAccuracy (val): 0.70\n",
    "# Epoch 18 \tLoss: 0.468 \tLoss (val): 0.697\tAccuracy: 0.78 \tAccuracy (val): 0.67\n",
    "# Epoch 19 \tLoss: 0.447 \tLoss (val): 0.598\tAccuracy: 0.79 \tAccuracy (val): 0.70\n",
    "# Epoch 20 \tLoss: 0.395 \tLoss (val): 0.588\tAccuracy: 0.83 \tAccuracy (val): 0.71\n",
    "# Epoch 21 \tLoss: 0.374 \tLoss (val): 0.614\tAccuracy: 0.84 \tAccuracy (val): 0.71\n",
    "# Epoch 22 \tLoss: 0.328 \tLoss (val): 0.662\tAccuracy: 0.86 \tAccuracy (val): 0.72\n",
    "# Epoch 23 \tLoss: 0.290 \tLoss (val): 0.688\tAccuracy: 0.88 \tAccuracy (val): 0.73\n",
    "# Epoch 24 \tLoss: 0.234 \tLoss (val): 0.693\tAccuracy: 0.90 \tAccuracy (val): 0.75\n",
    "# Epoch 25 \tLoss: 0.234 \tLoss (val): 0.762\tAccuracy: 0.91 \tAccuracy (val): 0.73\n",
    "# Epoch 26 \tLoss: 0.166 \tLoss (val): 0.945\tAccuracy: 0.93 \tAccuracy (val): 0.73\n",
    "# Epoch 27 \tLoss: 0.125 \tLoss (val): 1.202\tAccuracy: 0.96 \tAccuracy (val): 0.72\n",
    "# Epoch 28 \tLoss: 0.141 \tLoss (val): 0.968\tAccuracy: 0.94 \tAccuracy (val): 0.72\n",
    "# Epoch 29 \tLoss: 0.088 \tLoss (val): 1.162\tAccuracy: 0.97 \tAccuracy (val): 0.73\n",
    "# Epoch 0 \tLoss: 0.042 \tLoss (val): 1.622\tAccuracy: 0.99 \tAccuracy (val): 0.73\n",
    "# Epoch 1 \tLoss: 0.058 \tLoss (val): 1.725\tAccuracy: 0.98 \tAccuracy (val): 0.72\n",
    "# Epoch 2 \tLoss: 0.079 \tLoss (val): 1.250\tAccuracy: 0.98 \tAccuracy (val): 0.74\n",
    "# Epoch 3 \tLoss: 0.031 \tLoss (val): 1.508\tAccuracy: 0.99 \tAccuracy (val): 0.74\n",
    "# Epoch 4 \tLoss: 0.019 \tLoss (val): 1.837\tAccuracy: 0.99 \tAccuracy (val): 0.75\n",
    "# Epoch 5 \tLoss: 0.016 \tLoss (val): 1.502\tAccuracy: 1.00 \tAccuracy (val): 0.75\n",
    "# Epoch 6 \tLoss: 0.010 \tLoss (val): 1.917\tAccuracy: 1.00 \tAccuracy (val): 0.74\n",
    "# Epoch 7 \tLoss: 0.012 \tLoss (val): 2.114\tAccuracy: 1.00 \tAccuracy (val): 0.73\n",
    "# Epoch 8 \tLoss: 0.023 \tLoss (val): 1.930\tAccuracy: 0.99 \tAccuracy (val): 0.74\n",
    "# Epoch 9 \tLoss: 0.026 \tLoss (val): 1.862\tAccuracy: 0.99 \tAccuracy (val): 0.73\n",
    "# Epoch 10 \tLoss: 0.014 \tLoss (val): 2.444\tAccuracy: 0.99 \tAccuracy (val): 0.72\n",
    "# Epoch 11 \tLoss: 0.023 \tLoss (val): 2.080\tAccuracy: 0.99 \tAccuracy (val): 0.72\n",
    "# Epoch 12 \tLoss: 0.019 \tLoss (val): 1.901\tAccuracy: 0.99 \tAccuracy (val): 0.73\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#input 64x64x3\n",
    "class ImageClassifierNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3,padding=1)\n",
    "        #\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,padding=1)\n",
    "        #\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3,padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "       # self.conv7 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,padding=1)\n",
    "        \n",
    "        \n",
    "        self.conv8 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3,padding=1)\n",
    "        self.conv9 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3,padding=1)\n",
    "#         self.conv10 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3,padding=1)\n",
    "        \n",
    "#         self.conv11 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "     #   self.conv10_drop = nn.Dropout2d()\n",
    "    \n",
    "        self.fc1 = nn.Linear(4*4*512, 2)\n",
    "    #    self.fc2 = nn.Linear(150, 2)\n",
    "#         self.fc3 = nn.Linear(150, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #56   #32\n",
    "              \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #28   #16\n",
    "        \n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "      #  x = F.relu(self.conv7(x))     \n",
    "        x = F.max_pool2d(x, kernel_size=2) #14   #8\n",
    "        \n",
    "#         x = F.relu(self.conv11(x)) \n",
    "#         x = F.max_pool2d(x, kernel_size=2) #14   #4\n",
    "        \n",
    "        x = F.relu(self.conv8(x))\n",
    "        x = F.relu(self.conv9(x))\n",
    "\n",
    "#         x = F.relu(self.conv10(x)) \n",
    "#         x = F.relu(self.conv10_drop(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #4    #[14x14x512] if i=224x224    7x7x512 if i=112\n",
    "        \n",
    "   #     x = F.relu(self.conv11(x))\n",
    "   #     x = F.max_pool2d(x, kernel_size=2) #[7x7x512]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        x = x.view(-1, 4*4*512)\n",
    "      #  x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "       # x = F.dropout(x, training=self.training)\n",
    "        x = self.fc1(x)\n",
    "     #   return F.softmax(x, dim=1)\n",
    "        return x\n",
    "# Epoch 0 \tLoss: 0.695 \tLoss (val): 0.693\tAccuracy: 0.49 \tAccuracy (val): 0.50\n",
    "# Epoch 1 \tLoss: 0.693 \tLoss (val): 0.692\tAccuracy: 0.51 \tAccuracy (val): 0.50\n",
    "# Epoch 2 \tLoss: 0.680 \tLoss (val): 0.686\tAccuracy: 0.57 \tAccuracy (val): 0.57\n",
    "# Epoch 3 \tLoss: 0.661 \tLoss (val): 0.661\tAccuracy: 0.60 \tAccuracy (val): 0.61\n",
    "# Epoch 4 \tLoss: 0.621 \tLoss (val): 0.634\tAccuracy: 0.65 \tAccuracy (val): 0.65\n",
    "# Epoch 5 \tLoss: 0.588 \tLoss (val): 0.620\tAccuracy: 0.69 \tAccuracy (val): 0.64\n",
    "# Epoch 6 \tLoss: 0.586 \tLoss (val): 0.602\tAccuracy: 0.69 \tAccuracy (val): 0.67\n",
    "# Epoch 7 \tLoss: 0.576 \tLoss (val): 0.609\tAccuracy: 0.70 \tAccuracy (val): 0.67\n",
    "# Epoch 8 \tLoss: 0.567 \tLoss (val): 0.674\tAccuracy: 0.71 \tAccuracy (val): 0.63\n",
    "# Epoch 9 \tLoss: 0.552 \tLoss (val): 0.651\tAccuracy: 0.72 \tAccuracy (val): 0.64\n",
    "# Epoch 10 \tLoss: 0.552 \tLoss (val): 0.589\tAccuracy: 0.72 \tAccuracy (val): 0.67\n",
    "# Epoch 11 \tLoss: 0.517 \tLoss (val): 0.555\tAccuracy: 0.75 \tAccuracy (val): 0.71\n",
    "# Epoch 12 \tLoss: 0.501 \tLoss (val): 0.567\tAccuracy: 0.76 \tAccuracy (val): 0.69\n",
    "# Epoch 13 \tLoss: 0.486 \tLoss (val): 0.568\tAccuracy: 0.77 \tAccuracy (val): 0.70\n",
    "# Epoch 14 \tLoss: 0.482 \tLoss (val): 0.566\tAccuracy: 0.77 \tAccuracy (val): 0.72\n",
    "# Epoch 15 \tLoss: 0.456 \tLoss (val): 0.642\tAccuracy: 0.79 \tAccuracy (val): 0.66\n",
    "# Epoch 16 \tLoss: 0.438 \tLoss (val): 0.540\tAccuracy: 0.80 \tAccuracy (val): 0.75\n",
    "# Epoch 17 \tLoss: 0.415 \tLoss (val): 0.729\tAccuracy: 0.82 \tAccuracy (val): 0.67\n",
    "# Epoch 18 \tLoss: 0.395 \tLoss (val): 0.596\tAccuracy: 0.82 \tAccuracy (val): 0.73\n",
    "# Epoch 19 \tLoss: 0.352 \tLoss (val): 0.599\tAccuracy: 0.84 \tAccuracy (val): 0.74\n",
    "# Epoch 20 \tLoss: 0.301 \tLoss (val): 0.640\tAccuracy: 0.87 \tAccuracy (val): 0.74\n",
    "# Epoch 21 \tLoss: 0.263 \tLoss (val): 0.641\tAccuracy: 0.89 \tAccuracy (val): 0.74\n",
    "# Epoch 22 \tLoss: 0.232 \tLoss (val): 0.664\tAccuracy: 0.91 \tAccuracy (val): 0.72\n",
    "# Epoch 23 \tLoss: 0.183 \tLoss (val): 0.837\tAccuracy: 0.93 \tAccuracy (val): 0.73\n",
    "# Epoch 24 \tLoss: 0.151 \tLoss (val): 1.007\tAccuracy: 0.94 \tAccuracy (val): 0.72\n",
    "# Epoch 25 \tLoss: 0.119 \tLoss (val): 0.848\tAccuracy: 0.95 \tAccuracy (val): 0.73\n",
    "# Epoch 26 \tLoss: 0.118 \tLoss (val): 1.137\tAccuracy: 0.96 \tAccuracy (val): 0.71\n",
    "# Epoch 27 \tLoss: 0.136 \tLoss (val): 0.846\tAccuracy: 0.95 \tAccuracy (val): 0.76\n",
    "# Epoch 28 \tLoss: 0.064 \tLoss (val): 1.060\tAccuracy: 0.98 \tAccuracy (val): 0.75\n",
    "# Epoch 29 \tLoss: 0.077 \tLoss (val): 1.059\tAccuracy: 0.97 \tAccuracy (val): 0.75\n",
    "# Epoch 30 \tLoss: 0.076 \tLoss (val): 1.046\tAccuracy: 0.97 \tAccuracy (val): 0.76\n",
    "# Epoch 31 \tLoss: 0.028 \tLoss (val): 1.489\tAccuracy: 0.99 \tAccuracy (val): 0.75\n",
    "# Epoch 32 \tLoss: 0.030 \tLoss (val): 1.515\tAccuracy: 0.99 \tAccuracy (val): 0.76\n",
    "# Epoch 33 \tLoss: 0.048 \tLoss (val): 1.334\tAccuracy: 0.98 \tAccuracy (val): 0.74\n",
    "# Epoch 34 \tLoss: 0.039 \tLoss (val): 1.377\tAccuracy: 0.99 \tAccuracy (val): 0.75    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "class ImageClassifierNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3,padding=1)\n",
    "        #self.norm1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3,padding=1)\n",
    "       # self.norm2 = nn.BatchNorm2d(64)\n",
    "        #\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3,padding=1)\n",
    "     #   self.norm3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3,padding=1)\n",
    "     #   self.norm4 = nn.BatchNorm2d(128)\n",
    "        #\n",
    "        self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3,padding=1)\n",
    "      #  self.norm5 = nn.BatchNorm2d(256)\n",
    "        self.conv6 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,padding=1)\n",
    "      #  self.norm6 = nn.BatchNorm2d(256)\n",
    "\n",
    "        \n",
    "        self.fc1 = nn.Linear(8*8*128, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #56   #32\n",
    "              \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #28   #16\n",
    "        \n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x)) \n",
    "        x = F.max_pool2d(x, kernel_size=2) #14   #8      \n",
    "        \n",
    "        x = x.view(-1, 8*8*128)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x\n",
    "# Epoch 0 \tLoss: 0.695 \tLoss (val): 0.693\tAccuracy: 0.50 \tAccuracy (val): 0.50\n",
    "# Epoch 1 \tLoss: 0.692 \tLoss (val): 0.690\tAccuracy: 0.52 \tAccuracy (val): 0.55\n",
    "# Epoch 2 \tLoss: 0.675 \tLoss (val): 0.669\tAccuracy: 0.59 \tAccuracy (val): 0.59\n",
    "# Epoch 3 \tLoss: 0.660 \tLoss (val): 0.666\tAccuracy: 0.60 \tAccuracy (val): 0.59\n",
    "# Epoch 4 \tLoss: 0.632 \tLoss (val): 0.638\tAccuracy: 0.65 \tAccuracy (val): 0.63\n",
    "# Epoch 5 \tLoss: 0.606 \tLoss (val): 0.672\tAccuracy: 0.68 \tAccuracy (val): 0.64\n",
    "# Epoch 6 \tLoss: 0.591 \tLoss (val): 0.644\tAccuracy: 0.69 \tAccuracy (val): 0.64\n",
    "# Epoch 7 \tLoss: 0.562 \tLoss (val): 0.603\tAccuracy: 0.72 \tAccuracy (val): 0.68\n",
    "# Epoch 8 \tLoss: 0.529 \tLoss (val): 0.590\tAccuracy: 0.74 \tAccuracy (val): 0.68\n",
    "# Epoch 9 \tLoss: 0.500 \tLoss (val): 0.592\tAccuracy: 0.75 \tAccuracy (val): 0.71\n",
    "# Epoch 10 \tLoss: 0.477 \tLoss (val): 0.696\tAccuracy: 0.78 \tAccuracy (val): 0.67\n",
    "# Epoch 11 \tLoss: 0.468 \tLoss (val): 0.621\tAccuracy: 0.78 \tAccuracy (val): 0.69\n",
    "# Epoch 12 \tLoss: 0.435 \tLoss (val): 0.717\tAccuracy: 0.80 \tAccuracy (val): 0.67\n",
    "# Epoch 13 \tLoss: 0.415 \tLoss (val): 0.637\tAccuracy: 0.81 \tAccuracy (val): 0.71\n",
    "# Epoch 14 \tLoss: 0.365 \tLoss (val): 0.783\tAccuracy: 0.84 \tAccuracy (val): 0.68\n",
    "# Epoch 15 \tLoss: 0.345 \tLoss (val): 0.595\tAccuracy: 0.85 \tAccuracy (val): 0.72\n",
    "# Epoch 16 \tLoss: 0.304 \tLoss (val): 0.647\tAccuracy: 0.87 \tAccuracy (val): 0.72\n",
    "# Epoch 17 \tLoss: 0.255 \tLoss (val): 0.686\tAccuracy: 0.89 \tAccuracy (val): 0.73\n",
    "# Epoch 18 \tLoss: 0.205 \tLoss (val): 0.945\tAccuracy: 0.92 \tAccuracy (val): 0.70\n",
    "# Epoch 19 \tLoss: 0.168 \tLoss (val): 1.145\tAccuracy: 0.93 \tAccuracy (val): 0.69\n",
    "# Epoch 20 \tLoss: 0.129 \tLoss (val): 1.066\tAccuracy: 0.95 \tAccuracy (val): 0.72\n",
    "# Epoch 21 \tLoss: 0.099 \tLoss (val): 1.116\tAccuracy: 0.96 \tAccuracy (val): 0.73\n",
    "# Epoch 22 \tLoss: 0.066 \tLoss (val): 1.359\tAccuracy: 0.98 \tAccuracy (val): 0.72\n",
    "# Epoch 23 \tLoss: 0.066 \tLoss (val): 1.548\tAccuracy: 0.98 \tAccuracy (val): 0.69\n",
    "# Epoch 24 \tLoss: 0.057 \tLoss (val): 1.439\tAccuracy: 0.98 \tAccuracy (val): 0.72\n",
    "# Epoch 25 \tLoss: 0.048 \tLoss (val): 1.737\tAccuracy: 0.99 \tAccuracy (val): 0.71\n",
    "# Epoch 26 \tLoss: 0.023 \tLoss (val): 1.744\tAccuracy: 0.99 \tAccuracy (val): 0.73\n",
    "# Epoch 27 \tLoss: 0.013 \tLoss (val): 2.089\tAccuracy: 1.00 \tAccuracy (val): 0.71\n",
    "# Epoch 28 \tLoss: 0.007 \tLoss (val): 2.135\tAccuracy: 1.00 \tAccuracy (val): 0.71\n",
    "# Epoch 29 \tLoss: 0.003 \tLoss (val): 2.221\tAccuracy: 1.00 \tAccuracy (val): 0.72\n",
    "# Epoch 30 \tLoss: 0.001 \tLoss (val): 2.326\tAccuracy: 1.00 \tAccuracy (val): 0.72\n",
    "# Epoch 31 \tLoss: 0.001 \tLoss (val): 2.452\tAccuracy: 1.00 \tAccuracy (val): 0.72\n",
    "# Epoch 32 \tLoss: 0.000 \tLoss (val): 2.496\tAccuracy: 1.00 \tAccuracy (val): 0.72\n",
    "# Epoch 33 \tLoss: 0.000 \tLoss (val): 2.528\tAccuracy: 1.00 \tAccuracy (val): 0.72\n",
    "# Epoch 34 \tLoss: 0.000 \tLoss (val): 2.606\tAccuracy: 1.00 \tAccuracy (val): 0.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "class ImageClassifierNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3,padding=1)\n",
    "        #self.norm1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3,padding=1)\n",
    "       # self.norm2 = nn.BatchNorm2d(64)\n",
    "        #\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3,padding=1)\n",
    "     #   self.norm3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,padding=1)\n",
    "     #   self.norm4 = nn.BatchNorm2d(128)\n",
    "        #\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3,padding=1)\n",
    "      #  self.norm5 = nn.BatchNorm2d(256)\n",
    "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3,padding=1)\n",
    "      #  self.norm6 = nn.BatchNorm2d(256)\n",
    "\n",
    "        \n",
    "        self.fc1 = nn.Linear(8*8*512, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #56   #32\n",
    "              \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #28   #16\n",
    "        \n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x)) \n",
    "        x = F.max_pool2d(x, kernel_size=2) #14   #8\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = x.view(-1, 8*8*512)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "# Epoch 0 \tLoss: 0.696 \tLoss (val): 0.693\tAccuracy: 0.51 \tAccuracy (val): 0.49\n",
    "# Epoch 1 \tLoss: 0.681 \tLoss (val): 0.678\tAccuracy: 0.57 \tAccuracy (val): 0.56\n",
    "# Epoch 2 \tLoss: 0.652 \tLoss (val): 0.697\tAccuracy: 0.62 \tAccuracy (val): 0.57\n",
    "# Epoch 3 \tLoss: 0.613 \tLoss (val): 0.657\tAccuracy: 0.68 \tAccuracy (val): 0.60\n",
    "# Epoch 4 \tLoss: 0.575 \tLoss (val): 0.655\tAccuracy: 0.71 \tAccuracy (val): 0.62\n",
    "# Epoch 5 \tLoss: 0.544 \tLoss (val): 0.709\tAccuracy: 0.73 \tAccuracy (val): 0.64\n",
    "# Epoch 6 \tLoss: 0.515 \tLoss (val): 0.638\tAccuracy: 0.75 \tAccuracy (val): 0.68\n",
    "# Epoch 7 \tLoss: 0.541 \tLoss (val): 0.606\tAccuracy: 0.73 \tAccuracy (val): 0.68\n",
    "# Epoch 8 \tLoss: 0.473 \tLoss (val): 0.627\tAccuracy: 0.77 \tAccuracy (val): 0.68\n",
    "# Epoch 9 \tLoss: 0.438 \tLoss (val): 0.586\tAccuracy: 0.80 \tAccuracy (val): 0.70\n",
    "# Epoch 10 \tLoss: 0.398 \tLoss (val): 0.612\tAccuracy: 0.82 \tAccuracy (val): 0.71\n",
    "# Epoch 11 \tLoss: 0.382 \tLoss (val): 0.601\tAccuracy: 0.83 \tAccuracy (val): 0.73\n",
    "# Epoch 12 \tLoss: 0.329 \tLoss (val): 0.593\tAccuracy: 0.86 \tAccuracy (val): 0.75\n",
    "# Epoch 13 \tLoss: 0.297 \tLoss (val): 0.581\tAccuracy: 0.88 \tAccuracy (val): 0.75\n",
    "# Epoch 14 \tLoss: 0.249 \tLoss (val): 0.764\tAccuracy: 0.90 \tAccuracy (val): 0.68\n",
    "# Epoch 15 \tLoss: 0.213 \tLoss (val): 0.640\tAccuracy: 0.92 \tAccuracy (val): 0.75\n",
    "# Epoch 16 \tLoss: 0.149 \tLoss (val): 0.810\tAccuracy: 0.94 \tAccuracy (val): 0.74\n",
    "# Epoch 17 \tLoss: 0.107 \tLoss (val): 1.156\tAccuracy: 0.96 \tAccuracy (val): 0.70\n",
    "# Epoch 18 \tLoss: 0.114 \tLoss (val): 1.125\tAccuracy: 0.96 \tAccuracy (val): 0.70\n",
    "# Epoch 19 \tLoss: 0.060 \tLoss (val): 1.356\tAccuracy: 0.98 \tAccuracy (val): 0.71\n",
    "# Epoch 20 \tLoss: 0.041 \tLoss (val): 1.170\tAccuracy: 0.99 \tAccuracy (val): 0.74\n",
    "# Epoch 21 \tLoss: 0.030 \tLoss (val): 1.506\tAccuracy: 0.99 \tAccuracy (val): 0.73\n",
    "# Epoch 22 \tLoss: 0.025 \tLoss (val): 1.352\tAccuracy: 0.99 \tAccuracy (val): 0.75\n",
    "# Epoch 23 \tLoss: 0.040 \tLoss (val): 1.683\tAccuracy: 0.99 \tAccuracy (val): 0.69\n",
    "# Epoch 24 \tLoss: 0.024 \tLoss (val): 1.598\tAccuracy: 0.99 \tAccuracy (val): 0.71\n",
    "# Epoch 25 \tLoss: 0.011 \tLoss (val): 1.861\tAccuracy: 1.00 \tAccuracy (val): 0.72\n",
    "# Epoch 26 \tLoss: 0.007 \tLoss (val): 1.989\tAccuracy: 1.00 \tAccuracy (val): 0.72\n",
    "# Epoch 27 \tLoss: 0.002 \tLoss (val): 2.087\tAccuracy: 1.00 \tAccuracy (val): 0.72\n",
    "# Epoch 28 \tLoss: 0.001 \tLoss (val): 2.098\tAccuracy: 1.00 \tAccuracy (val): 0.72\n",
    "# Epoch 29 \tLoss: 0.000 \tLoss (val): 2.190\tAccuracy: 1.00 \tAccuracy (val): 0.72\n",
    "# Epoch 30 \tLoss: 0.000 \tLoss (val): 2.216\tAccuracy: 1.00 \tAccuracy (val): 0.72\n",
    "# Epoch 31 \tLoss: 0.000 \tLoss (val): 2.305\tAccuracy: 1.00 \tAccuracy (val): 0.72\n",
    "# Epoch 32 \tLoss: 0.000 \tLoss (val): 2.420\tAccuracy: 1.00 \tAccuracy (val): 0.72\n",
    "# Epoch 33 \tLoss: 0.000 \tLoss (val): 2.440\tAccuracy: 1.00 \tAccuracy (val): 0.72\n",
    "# Epoch 34 \tLoss: 0.000 \tLoss (val): 2.519\tAccuracy: 1.00 \tAccuracy (val): 0.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "class ImageClassifierNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3,padding=1)\n",
    "        self.norm1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3,padding=1)\n",
    "        self.norm2 = nn.BatchNorm2d(64)\n",
    "        #\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3,padding=1)\n",
    "        self.norm3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,padding=1)\n",
    "        self.norm4 = nn.BatchNorm2d(128)\n",
    "        #\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3,padding=1)\n",
    "        self.norm5 = nn.BatchNorm2d(256)\n",
    "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "        self.norm6 = nn.BatchNorm2d(256)\n",
    "\n",
    "        \n",
    "        self.fc1 = nn.Linear(8*8*256, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.norm1(self.conv1(x)))\n",
    "        x = F.relu(self.norm2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #56   #32\n",
    "              \n",
    "        x = F.relu(self.norm3(self.conv3(x)))\n",
    "        x = F.relu(self.norm4(self.conv4(x)))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #28   #16\n",
    "        \n",
    "        x = F.relu(self.norm5(self.conv5(x)))\n",
    "        x = F.relu(self.norm6(self.conv6(x))) \n",
    "        x = F.max_pool2d(x, kernel_size=2) #14   #8\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = x.view(-1, 8*8*256)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x\n",
    "# Epoch 0 \tLoss: 1.352 \tLoss (val): 0.795\tAccuracy: 0.55 \tAccuracy (val): 0.58\n",
    "# Epoch 1 \tLoss: 0.947 \tLoss (val): 0.826\tAccuracy: 0.60 \tAccuracy (val): 0.57\n",
    "# Epoch 2 \tLoss: 0.877 \tLoss (val): 0.868\tAccuracy: 0.60 \tAccuracy (val): 0.56\n",
    "# Epoch 3 \tLoss: 0.708 \tLoss (val): 0.829\tAccuracy: 0.66 \tAccuracy (val): 0.57\n",
    "# Epoch 4 \tLoss: 0.737 \tLoss (val): 0.802\tAccuracy: 0.69 \tAccuracy (val): 0.61\n",
    "# Epoch 5 \tLoss: 0.599 \tLoss (val): 0.711\tAccuracy: 0.70 \tAccuracy (val): 0.62\n",
    "# Epoch 6 \tLoss: 0.600 \tLoss (val): 0.864\tAccuracy: 0.73 \tAccuracy (val): 0.59\n",
    "# Epoch 7 \tLoss: 0.616 \tLoss (val): 0.823\tAccuracy: 0.71 \tAccuracy (val): 0.62\n",
    "# Epoch 8 \tLoss: 0.562 \tLoss (val): 0.716\tAccuracy: 0.75 \tAccuracy (val): 0.63\n",
    "# Epoch 9 \tLoss: 0.534 \tLoss (val): 0.738\tAccuracy: 0.75 \tAccuracy (val): 0.64\n",
    "# Epoch 10 \tLoss: 0.497 \tLoss (val): 0.743\tAccuracy: 0.77 \tAccuracy (val): 0.64\n",
    "# Epoch 11 \tLoss: 0.474 \tLoss (val): 0.672\tAccuracy: 0.79 \tAccuracy (val): 0.67\n",
    "# Epoch 12 \tLoss: 0.460 \tLoss (val): 0.719\tAccuracy: 0.79 \tAccuracy (val): 0.64\n",
    "# Epoch 13 \tLoss: 0.423 \tLoss (val): 0.739\tAccuracy: 0.81 \tAccuracy (val): 0.66\n",
    "# Epoch 14 \tLoss: 0.364 \tLoss (val): 0.794\tAccuracy: 0.83 \tAccuracy (val): 0.66\n",
    "# Epoch 15 \tLoss: 0.375 \tLoss (val): 0.708\tAccuracy: 0.84 \tAccuracy (val): 0.68\n",
    "# Epoch 16 \tLoss: 0.319 \tLoss (val): 0.888\tAccuracy: 0.86 \tAccuracy (val): 0.65\n",
    "# Epoch 17 \tLoss: 0.354 \tLoss (val): 0.776\tAccuracy: 0.84 \tAccuracy (val): 0.67\n",
    "# Epoch 18 \tLoss: 0.333 \tLoss (val): 0.926\tAccuracy: 0.86 \tAccuracy (val): 0.67\n",
    "# Epoch 19 \tLoss: 0.303 \tLoss (val): 0.798\tAccuracy: 0.88 \tAccuracy (val): 0.70\n",
    "# Epoch 20 \tLoss: 0.258 \tLoss (val): 0.878\tAccuracy: 0.88 \tAccuracy (val): 0.68\n",
    "# Epoch 21 \tLoss: 0.243 \tLoss (val): 0.851\tAccuracy: 0.90 \tAccuracy (val): 0.69\n",
    "# Epoch 22 \tLoss: 0.273 \tLoss (val): 1.079\tAccuracy: 0.88 \tAccuracy (val): 0.66\n",
    "# Epoch 23 \tLoss: 0.183 \tLoss (val): 1.087\tAccuracy: 0.92 \tAccuracy (val): 0.68\n",
    "# Epoch 24 \tLoss: 0.140 \tLoss (val): 1.125\tAccuracy: 0.94 \tAccuracy (val): 0.67\n",
    "# Epoch 25 \tLoss: 0.150 \tLoss (val): 1.232\tAccuracy: 0.94 \tAccuracy (val): 0.68\n",
    "# Epoch 26 \tLoss: 0.147 \tLoss (val): 1.141\tAccuracy: 0.94 \tAccuracy (val): 0.68\n",
    "# Epoch 27 \tLoss: 0.113 \tLoss (val): 1.408\tAccuracy: 0.96 \tAccuracy (val): 0.67\n",
    "# Epoch 28 \tLoss: 0.115 \tLoss (val): 1.166\tAccuracy: 0.95 \tAccuracy (val): 0.70\n",
    "# Epoch 29 \tLoss: 0.057 \tLoss (val): 1.392\tAccuracy: 0.98 \tAccuracy (val): 0.69\n",
    "# Epoch 30 \tLoss: 0.042 \tLoss (val): 1.301\tAccuracy: 0.99 \tAccuracy (val): 0.69\n",
    "# Epoch 31 \tLoss: 0.030 \tLoss (val): 1.433\tAccuracy: 0.99 \tAccuracy (val): 0.71\n",
    "# Epoch 32 \tLoss: 0.029 \tLoss (val): 1.520\tAccuracy: 0.99 \tAccuracy (val): 0.70\n",
    "# Epoch 33 \tLoss: 0.032 \tLoss (val): 1.414\tAccuracy: 0.99 \tAccuracy (val): 0.70\n",
    "# Epoch 34 \tLoss: 0.029 \tLoss (val): 1.432\tAccuracy: 0.99 \tAccuracy (val): 0.69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "class ImageClassifierNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3,padding=1)\n",
    "        #\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,padding=1)\n",
    "        #\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3,padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "        self.conv7 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(8*8*256, 150)\n",
    "        self.fc2 = nn.Linear(150, 2)\n",
    "      #  self.fc3 = nn.Linear(150, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #56   #32\n",
    "              \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #28   #16\n",
    "        \n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = F.relu(self.conv7(x))     \n",
    "        x = F.max_pool2d(x, kernel_size=2) #14   #8\n",
    "        \n",
    "       \n",
    "        x = x.view(-1, 8*8*256)\n",
    "        x = F.relu(self.fc1(x))\n",
    "       # x = F.relu(self.fc2(x))\n",
    "      #  x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "     #   return F.softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "# Epoch 0 \tLoss: 0.695 \tLoss (val): 0.693\tAccuracy: 0.50 \tAccuracy (val): 0.50\n",
    "# Epoch 1 \tLoss: 0.693 \tLoss (val): 0.691\tAccuracy: 0.51 \tAccuracy (val): 0.52\n",
    "# Epoch 2 \tLoss: 0.691 \tLoss (val): 0.690\tAccuracy: 0.54 \tAccuracy (val): 0.54\n",
    "# Epoch 3 \tLoss: 0.676 \tLoss (val): 0.682\tAccuracy: 0.58 \tAccuracy (val): 0.56\n",
    "# Epoch 4 \tLoss: 0.645 \tLoss (val): 0.664\tAccuracy: 0.62 \tAccuracy (val): 0.58\n",
    "# Epoch 5 \tLoss: 0.614 \tLoss (val): 0.635\tAccuracy: 0.66 \tAccuracy (val): 0.63\n",
    "# Epoch 6 \tLoss: 0.610 \tLoss (val): 0.631\tAccuracy: 0.68 \tAccuracy (val): 0.63\n",
    "# Epoch 7 \tLoss: 0.561 \tLoss (val): 0.626\tAccuracy: 0.71 \tAccuracy (val): 0.66\n",
    "# Epoch 8 \tLoss: 0.531 \tLoss (val): 0.605\tAccuracy: 0.73 \tAccuracy (val): 0.68\n",
    "# Epoch 9 \tLoss: 0.493 \tLoss (val): 0.604\tAccuracy: 0.76 \tAccuracy (val): 0.69\n",
    "# Epoch 10 \tLoss: 0.484 \tLoss (val): 0.582\tAccuracy: 0.77 \tAccuracy (val): 0.70\n",
    "# Epoch 11 \tLoss: 0.427 \tLoss (val): 0.665\tAccuracy: 0.81 \tAccuracy (val): 0.70\n",
    "# Epoch 12 \tLoss: 0.397 \tLoss (val): 0.579\tAccuracy: 0.81 \tAccuracy (val): 0.72\n",
    "# Epoch 13 \tLoss: 0.337 \tLoss (val): 0.591\tAccuracy: 0.85 \tAccuracy (val): 0.72\n",
    "# Epoch 14 \tLoss: 0.314 \tLoss (val): 0.620\tAccuracy: 0.86 \tAccuracy (val): 0.70\n",
    "# Epoch 15 \tLoss: 0.265 \tLoss (val): 0.655\tAccuracy: 0.89 \tAccuracy (val): 0.71\n",
    "# Epoch 16 \tLoss: 0.226 \tLoss (val): 0.815\tAccuracy: 0.91 \tAccuracy (val): 0.73\n",
    "# Epoch 17 \tLoss: 0.185 \tLoss (val): 0.935\tAccuracy: 0.92 \tAccuracy (val): 0.73\n",
    "# Epoch 18 \tLoss: 0.132 \tLoss (val): 1.121\tAccuracy: 0.95 \tAccuracy (val): 0.70\n",
    "# Epoch 19 \tLoss: 0.136 \tLoss (val): 1.048\tAccuracy: 0.95 \tAccuracy (val): 0.72\n",
    "# Epoch 20 \tLoss: 0.066 \tLoss (val): 1.563\tAccuracy: 0.98 \tAccuracy (val): 0.71\n",
    "# Epoch 21 \tLoss: 0.093 \tLoss (val): 1.456\tAccuracy: 0.97 \tAccuracy (val): 0.71\n",
    "# Epoch 22 \tLoss: 0.077 \tLoss (val): 1.791\tAccuracy: 0.97 \tAccuracy (val): 0.68\n",
    "# Epoch 23 \tLoss: 0.070 \tLoss (val): 1.805\tAccuracy: 0.98 \tAccuracy (val): 0.70\n",
    "# Epoch 24 \tLoss: 0.028 \tLoss (val): 2.163\tAccuracy: 0.99 \tAccuracy (val): 0.72\n",
    "# Epoch 25 \tLoss: 0.035 \tLoss (val): 1.731\tAccuracy: 0.99 \tAccuracy (val): 0.72\n",
    "# Epoch 26 \tLoss: 0.042 \tLoss (val): 1.908\tAccuracy: 0.98 \tAccuracy (val): 0.71\n",
    "# Epoch 27 \tLoss: 0.038 \tLoss (val): 1.878\tAccuracy: 0.99 \tAccuracy (val): 0.71\n",
    "# Epoch 28 \tLoss: 0.025 \tLoss (val): 1.846\tAccuracy: 0.99 \tAccuracy (val): 0.73\n",
    "# Epoch 29 \tLoss: 0.010 \tLoss (val): 2.827\tAccuracy: 1.00 \tAccuracy (val): 0.72\n",
    "# Epoch 30 \tLoss: 0.026 \tLoss (val): 3.072\tAccuracy: 0.99 \tAccuracy (val): 0.69\n",
    "# Epoch 31 \tLoss: 0.043 \tLoss (val): 2.138\tAccuracy: 0.98 \tAccuracy (val): 0.70\n",
    "# Epoch 32 \tLoss: 0.019 \tLoss (val): 2.058\tAccuracy: 0.99 \tAccuracy (val): 0.74\n",
    "# Epoch 33 \tLoss: 0.008 \tLoss (val): 2.608\tAccuracy: 1.00 \tAccuracy (val): 0.71\n",
    "# Epoch 34 \tLoss: 0.001 \tLoss (val): 2.496\tAccuracy: 1.00 \tAccuracy (val): 0.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "class ImageClassifierNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3,padding=1)\n",
    "        #\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,padding=1)\n",
    "        #\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3,padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "        self.conv7 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "        \n",
    "        \n",
    "        self.conv8 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3,padding=1)\n",
    "        self.conv9 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3,padding=1)\n",
    "        self.conv10 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3,padding=1)\n",
    "        \n",
    "#         self.conv11 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3,padding=1)\n",
    "      #  self.conv10_drop = nn.Dropout2d()\n",
    "    \n",
    "        self.fc1 = nn.Linear(4*4*512, 150)\n",
    "        self.fc2 = nn.Linear(150, 2)\n",
    "      #  self.fc3 = nn.Linear(150, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #56   #32\n",
    "              \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #28   #16\n",
    "        \n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = F.relu(self.conv7(x))     \n",
    "        x = F.max_pool2d(x, kernel_size=2) #14   #8\n",
    "        \n",
    "        \n",
    "        x = F.relu(self.conv8(x))\n",
    "        x = F.relu(self.conv9(x))\n",
    "        x = F.relu(self.conv10(x)) \n",
    "    #    x = F.relu(self.conv10_drop(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #4    #[14x14x512] if i=224x224    7x7x512 if i=112\n",
    "        \n",
    "   #     x = F.relu(self.conv11(x))\n",
    "   #     x = F.max_pool2d(x, kernel_size=2) #[7x7x512]\n",
    "        \n",
    "        x = x.view(-1, 4*4*512)\n",
    "        x = F.relu(self.fc1(x))\n",
    "       # x = F.relu(self.fc2(x))\n",
    "      #  x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "     #   return F.softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "# Epoch 0 \tLoss: 0.694 \tLoss (val): 0.693\tAccuracy: 0.50 \tAccuracy (val): 0.50\n",
    "# Epoch 1 \tLoss: 0.693 \tLoss (val): 0.693\tAccuracy: 0.50 \tAccuracy (val): 0.50\n",
    "# Epoch 2 \tLoss: 0.693 \tLoss (val): 0.693\tAccuracy: 0.50 \tAccuracy (val): 0.50\n",
    "# Epoch 3 \tLoss: 0.693 \tLoss (val): 0.693\tAccuracy: 0.50 \tAccuracy (val): 0.50\n",
    "# Epoch 4 \tLoss: 0.697 \tLoss (val): 0.693\tAccuracy: 0.51 \tAccuracy (val): 0.50\n",
    "# Epoch 5 \tLoss: 0.693 \tLoss (val): 0.693\tAccuracy: 0.50 \tAccuracy (val): 0.50\n",
    "# Epoch 6 \tLoss: 0.693 \tLoss (val): 0.693\tAccuracy: 0.50 \tAccuracy (val): 0.50\n",
    "# Epoch 7 \tLoss: 0.693 \tLoss (val): 0.693\tAccuracy: 0.50 \tAccuracy (val): 0.50    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifierNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3,padding=1)\n",
    "        #\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,padding=1)\n",
    "        #\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3,padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "    \n",
    "        self.conv7 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "        self.conv8 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(4*4*256, 2)\n",
    "   #     self.fc2 = nn.Linear(150, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #56   #32\n",
    "              \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #28   #16\n",
    "        \n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #14   #8\n",
    "       \n",
    "        x = F.relu(self.conv7(x))\n",
    "        x = F.relu(self.conv8(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #14   #4\n",
    "    \n",
    "        \n",
    "        x = x.view(-1, 4*4*256)\n",
    "        x = self.fc1(x)\n",
    "    #    x = self.fc2(x)\n",
    "        return x\n",
    "# Epoch 0 \tLoss: 0.694 \tLoss (val): 0.692\tAccuracy: 0.50 \tAccuracy (val): 0.53\n",
    "# Epoch 1 \tLoss: 0.692 \tLoss (val): 0.693\tAccuracy: 0.53 \tAccuracy (val): 0.51\n",
    "# Epoch 2 \tLoss: 0.689 \tLoss (val): 0.691\tAccuracy: 0.53 \tAccuracy (val): 0.54\n",
    "# Epoch 3 \tLoss: 0.682 \tLoss (val): 0.691\tAccuracy: 0.55 \tAccuracy (val): 0.52\n",
    "# Epoch 4 \tLoss: 0.668 \tLoss (val): 0.661\tAccuracy: 0.60 \tAccuracy (val): 0.59\n",
    "# Epoch 5 \tLoss: 0.637 \tLoss (val): 0.652\tAccuracy: 0.64 \tAccuracy (val): 0.61\n",
    "# Epoch 6 \tLoss: 0.610 \tLoss (val): 0.686\tAccuracy: 0.67 \tAccuracy (val): 0.61\n",
    "# Epoch 7 \tLoss: 0.585 \tLoss (val): 0.598\tAccuracy: 0.69 \tAccuracy (val): 0.67\n",
    "# Epoch 8 \tLoss: 0.564 \tLoss (val): 0.651\tAccuracy: 0.71 \tAccuracy (val): 0.64\n",
    "# Epoch 9 \tLoss: 0.556 \tLoss (val): 0.579\tAccuracy: 0.72 \tAccuracy (val): 0.70\n",
    "# Epoch 10 \tLoss: 0.532 \tLoss (val): 0.629\tAccuracy: 0.73 \tAccuracy (val): 0.69\n",
    "# Epoch 11 \tLoss: 0.500 \tLoss (val): 0.557\tAccuracy: 0.76 \tAccuracy (val): 0.71\n",
    "# Epoch 12 \tLoss: 0.474 \tLoss (val): 0.564\tAccuracy: 0.78 \tAccuracy (val): 0.72\n",
    "# Epoch 13 \tLoss: 0.446 \tLoss (val): 0.584\tAccuracy: 0.80 \tAccuracy (val): 0.71\n",
    "# Epoch 14 \tLoss: 0.404 \tLoss (val): 0.702\tAccuracy: 0.82 \tAccuracy (val): 0.69\n",
    "# Epoch 15 \tLoss: 0.382 \tLoss (val): 0.660\tAccuracy: 0.83 \tAccuracy (val): 0.70\n",
    "# Epoch 16 \tLoss: 0.348 \tLoss (val): 0.513\tAccuracy: 0.85 \tAccuracy (val): 0.75\n",
    "# Epoch 17 \tLoss: 0.310 \tLoss (val): 0.574\tAccuracy: 0.87 \tAccuracy (val): 0.75\n",
    "# Epoch 18 \tLoss: 0.265 \tLoss (val): 1.013\tAccuracy: 0.89 \tAccuracy (val): 0.67\n",
    "# Epoch 19 \tLoss: 0.235 \tLoss (val): 1.022\tAccuracy: 0.90 \tAccuracy (val): 0.69\n",
    "# Epoch 20 \tLoss: 0.179 \tLoss (val): 0.955\tAccuracy: 0.93 \tAccuracy (val): 0.71\n",
    "# Epoch 21 \tLoss: 0.122 \tLoss (val): 0.859\tAccuracy: 0.95 \tAccuracy (val): 0.76\n",
    "# Epoch 22 \tLoss: 0.129 \tLoss (val): 0.925\tAccuracy: 0.95 \tAccuracy (val): 0.75\n",
    "# Epoch 23 \tLoss: 0.080 \tLoss (val): 1.147\tAccuracy: 0.97 \tAccuracy (val): 0.74\n",
    "# Epoch 24 \tLoss: 0.064 \tLoss (val): 1.083\tAccuracy: 0.98 \tAccuracy (val): 0.72\n",
    "# Epoch 25 \tLoss: 0.088 \tLoss (val): 0.944\tAccuracy: 0.97 \tAccuracy (val): 0.76\n",
    "# Epoch 26 \tLoss: 0.045 \tLoss (val): 1.338\tAccuracy: 0.98 \tAccuracy (val): 0.73\n",
    "# Epoch 27 \tLoss: 0.050 \tLoss (val): 1.459\tAccuracy: 0.98 \tAccuracy (val): 0.74\n",
    "# Epoch 28 \tLoss: 0.031 \tLoss (val): 1.541\tAccuracy: 0.99 \tAccuracy (val): 0.74\n",
    "# Epoch 29 \tLoss: 0.015 \tLoss (val): 1.583\tAccuracy: 0.99 \tAccuracy (val): 0.74\n",
    "# Epoch 30 \tLoss: 0.011 \tLoss (val): 1.800\tAccuracy: 1.00 \tAccuracy (val): 0.76\n",
    "# Epoch 31 \tLoss: 0.015 \tLoss (val): 1.715\tAccuracy: 0.99 \tAccuracy (val): 0.73\n",
    "# Epoch 32 \tLoss: 0.040 \tLoss (val): 1.490\tAccuracy: 0.99 \tAccuracy (val): 0.73\n",
    "# Epoch 33 \tLoss: 0.041 \tLoss (val): 2.140\tAccuracy: 0.98 \tAccuracy (val): 0.70\n",
    "# Epoch 34 \tLoss: 0.065 \tLoss (val): 1.465\tAccuracy: 0.98 \tAccuracy (val): 0.71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifierNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3,padding=1)\n",
    "        #\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,padding=1)\n",
    "        #\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3,padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "    \n",
    "        self.conv7 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "        self.conv8 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(4*4*256, 150)\n",
    "        self.fc2 = nn.Linear(150, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #56   #32\n",
    "              \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #28   #16\n",
    "        \n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #14   #8\n",
    "       \n",
    "        x = F.relu(self.conv7(x))\n",
    "        x = F.relu(self.conv8(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #14   #4\n",
    "    \n",
    "        \n",
    "        x = x.view(-1, 4*4*256)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "# Epoch 0 \tLoss: 0.698 \tLoss (val): 0.693\tAccuracy: 0.50 \tAccuracy (val): 0.50\n",
    "# Epoch 1 \tLoss: 0.692 \tLoss (val): 0.697\tAccuracy: 0.54 \tAccuracy (val): 0.49\n",
    "# Epoch 2 \tLoss: 0.683 \tLoss (val): 0.695\tAccuracy: 0.55 \tAccuracy (val): 0.50\n",
    "# Epoch 3 \tLoss: 0.672 \tLoss (val): 0.692\tAccuracy: 0.58 \tAccuracy (val): 0.54\n",
    "# Epoch 4 \tLoss: 0.650 \tLoss (val): 0.695\tAccuracy: 0.62 \tAccuracy (val): 0.57\n",
    "# Epoch 5 \tLoss: 0.624 \tLoss (val): 0.658\tAccuracy: 0.65 \tAccuracy (val): 0.58\n",
    "# Epoch 6 \tLoss: 0.621 \tLoss (val): 0.650\tAccuracy: 0.66 \tAccuracy (val): 0.62\n",
    "# Epoch 7 \tLoss: 0.596 \tLoss (val): 0.653\tAccuracy: 0.68 \tAccuracy (val): 0.63\n",
    "# Epoch 8 \tLoss: 0.587 \tLoss (val): 0.617\tAccuracy: 0.69 \tAccuracy (val): 0.65\n",
    "# Epoch 9 \tLoss: 0.560 \tLoss (val): 0.633\tAccuracy: 0.72 \tAccuracy (val): 0.64\n",
    "# Epoch 10 \tLoss: 0.563 \tLoss (val): 0.596\tAccuracy: 0.70 \tAccuracy (val): 0.68\n",
    "# Epoch 11 \tLoss: 0.544 \tLoss (val): 0.659\tAccuracy: 0.72 \tAccuracy (val): 0.65\n",
    "# Epoch 12 \tLoss: 0.554 \tLoss (val): 0.591\tAccuracy: 0.72 \tAccuracy (val): 0.69\n",
    "# Epoch 13 \tLoss: 0.516 \tLoss (val): 0.560\tAccuracy: 0.75 \tAccuracy (val): 0.73\n",
    "# Epoch 14 \tLoss: 0.494 \tLoss (val): 0.641\tAccuracy: 0.76 \tAccuracy (val): 0.67\n",
    "# Epoch 15 \tLoss: 0.470 \tLoss (val): 0.634\tAccuracy: 0.78 \tAccuracy (val): 0.68\n",
    "# Epoch 16 \tLoss: 0.452 \tLoss (val): 0.585\tAccuracy: 0.79 \tAccuracy (val): 0.71\n",
    "# Epoch 17 \tLoss: 0.406 \tLoss (val): 0.611\tAccuracy: 0.82 \tAccuracy (val): 0.71\n",
    "# Epoch 18 \tLoss: 0.395 \tLoss (val): 0.641\tAccuracy: 0.83 \tAccuracy (val): 0.72\n",
    "# Epoch 19 \tLoss: 0.376 \tLoss (val): 0.649\tAccuracy: 0.84 \tAccuracy (val): 0.71\n",
    "# Epoch 20 \tLoss: 0.307 \tLoss (val): 0.774\tAccuracy: 0.86 \tAccuracy (val): 0.71\n",
    "# Epoch 21 \tLoss: 0.271 \tLoss (val): 0.730\tAccuracy: 0.89 \tAccuracy (val): 0.72\n",
    "# Epoch 22 \tLoss: 0.214 \tLoss (val): 0.831\tAccuracy: 0.91 \tAccuracy (val): 0.73\n",
    "# Epoch 23 \tLoss: 0.212 \tLoss (val): 0.786\tAccuracy: 0.91 \tAccuracy (val): 0.72\n",
    "# Epoch 24 \tLoss: 0.152 \tLoss (val): 0.913\tAccuracy: 0.94 \tAccuracy (val): 0.73\n",
    "# Epoch 25 \tLoss: 0.098 \tLoss (val): 1.110\tAccuracy: 0.96 \tAccuracy (val): 0.72\n",
    "# Epoch 26 \tLoss: 0.094 \tLoss (val): 1.145\tAccuracy: 0.96 \tAccuracy (val): 0.71\n",
    "# Epoch 27 \tLoss: 0.055 \tLoss (val): 1.323\tAccuracy: 0.98 \tAccuracy (val): 0.73\n",
    "# Epoch 28 \tLoss: 0.053 \tLoss (val): 1.680\tAccuracy: 0.98 \tAccuracy (val): 0.71\n",
    "# Epoch 29 \tLoss: 0.085 \tLoss (val): 1.433\tAccuracy: 0.97 \tAccuracy (val): 0.68\n",
    "# Epoch 30 \tLoss: 0.073 \tLoss (val): 1.505\tAccuracy: 0.97 \tAccuracy (val): 0.72\n",
    "# Epoch 31 \tLoss: 0.051 \tLoss (val): 1.589\tAccuracy: 0.98 \tAccuracy (val): 0.72\n",
    "# Epoch 32 \tLoss: 0.054 \tLoss (val): 1.460\tAccuracy: 0.98 \tAccuracy (val): 0.73\n",
    "# Epoch 33 \tLoss: 0.039 \tLoss (val): 1.627\tAccuracy: 0.98 \tAccuracy (val): 0.72\n",
    "# Epoch 34 \tLoss: 0.025 \tLoss (val): 1.955\tAccuracy: 0.99 \tAccuracy (val): 0.73    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifierNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3,padding=1)\n",
    "        #\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,padding=1)\n",
    "        #\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3,padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "    \n",
    "        self.fc1 = nn.Linear(8*8*256, 150)\n",
    "        self.fc2 = nn.Linear(150, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #56   #32\n",
    "              \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #28   #16\n",
    "        \n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #14   #8\n",
    "       \n",
    "        \n",
    "        x = x.view(-1, 8*8*256)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "# Epoch 0 \tLoss: 0.690 \tLoss (val): 0.705\tAccuracy: 0.53 \tAccuracy (val): 0.52\n",
    "# Epoch 1 \tLoss: 0.693 \tLoss (val): 0.692\tAccuracy: 0.53 \tAccuracy (val): 0.51\n",
    "# Epoch 2 \tLoss: 0.674 \tLoss (val): 0.678\tAccuracy: 0.57 \tAccuracy (val): 0.58\n",
    "# Epoch 3 \tLoss: 0.657 \tLoss (val): 0.673\tAccuracy: 0.63 \tAccuracy (val): 0.60\n",
    "# Epoch 4 \tLoss: 0.664 \tLoss (val): 0.729\tAccuracy: 0.58 \tAccuracy (val): 0.62\n",
    "# Epoch 5 \tLoss: 0.621 \tLoss (val): 0.641\tAccuracy: 0.65 \tAccuracy (val): 0.61\n",
    "# Epoch 6 \tLoss: 0.601 \tLoss (val): 0.648\tAccuracy: 0.68 \tAccuracy (val): 0.61\n",
    "# Epoch 7 \tLoss: 0.594 \tLoss (val): 0.618\tAccuracy: 0.69 \tAccuracy (val): 0.65\n",
    "# Epoch 8 \tLoss: 0.575 \tLoss (val): 0.612\tAccuracy: 0.70 \tAccuracy (val): 0.66\n",
    "# Epoch 9 \tLoss: 0.564 \tLoss (val): 0.609\tAccuracy: 0.71 \tAccuracy (val): 0.67\n",
    "# Epoch 10 \tLoss: 0.539 \tLoss (val): 0.653\tAccuracy: 0.72 \tAccuracy (val): 0.66\n",
    "# Epoch 11 \tLoss: 0.520 \tLoss (val): 0.591\tAccuracy: 0.74 \tAccuracy (val): 0.69\n",
    "# Epoch 12 \tLoss: 0.497 \tLoss (val): 0.574\tAccuracy: 0.76 \tAccuracy (val): 0.70\n",
    "# Epoch 13 \tLoss: 0.477 \tLoss (val): 0.632\tAccuracy: 0.77 \tAccuracy (val): 0.69\n",
    "# Epoch 14 \tLoss: 0.459 \tLoss (val): 0.645\tAccuracy: 0.78 \tAccuracy (val): 0.67\n",
    "# Epoch 15 \tLoss: 0.435 \tLoss (val): 0.579\tAccuracy: 0.79 \tAccuracy (val): 0.72\n",
    "# Epoch 16 \tLoss: 0.365 \tLoss (val): 0.617\tAccuracy: 0.84 \tAccuracy (val): 0.71\n",
    "# Epoch 17 \tLoss: 0.343 \tLoss (val): 0.630\tAccuracy: 0.85 \tAccuracy (val): 0.71\n",
    "# Epoch 18 \tLoss: 0.295 \tLoss (val): 0.720\tAccuracy: 0.87 \tAccuracy (val): 0.69\n",
    "# Epoch 19 \tLoss: 0.221 \tLoss (val): 0.736\tAccuracy: 0.91 \tAccuracy (val): 0.72\n",
    "# Epoch 20 \tLoss: 0.177 \tLoss (val): 0.779\tAccuracy: 0.93 \tAccuracy (val): 0.71\n",
    "# Epoch 21 \tLoss: 0.157 \tLoss (val): 0.926\tAccuracy: 0.94 \tAccuracy (val): 0.71\n",
    "# Epoch 22 \tLoss: 0.108 \tLoss (val): 1.157\tAccuracy: 0.96 \tAccuracy (val): 0.73\n",
    "# Epoch 23 \tLoss: 0.064 \tLoss (val): 1.618\tAccuracy: 0.98 \tAccuracy (val): 0.70\n",
    "# Epoch 24 \tLoss: 0.102 \tLoss (val): 1.270\tAccuracy: 0.96 \tAccuracy (val): 0.72\n",
    "# Epoch 25 \tLoss: 0.052 \tLoss (val): 1.396\tAccuracy: 0.98 \tAccuracy (val): 0.72\n",
    "# Epoch 26 \tLoss: 0.045 \tLoss (val): 1.512\tAccuracy: 0.98 \tAccuracy (val): 0.73\n",
    "# Epoch 27 \tLoss: 0.041 \tLoss (val): 1.711\tAccuracy: 0.99 \tAccuracy (val): 0.70\n",
    "# Epoch 28 \tLoss: 0.046 \tLoss (val): 1.416\tAccuracy: 0.99 \tAccuracy (val): 0.71\n",
    "# Epoch 29 \tLoss: 0.092 \tLoss (val): 1.305\tAccuracy: 0.97 \tAccuracy (val): 0.73\n",
    "# Epoch 30 \tLoss: 0.022 \tLoss (val): 1.674\tAccuracy: 0.99 \tAccuracy (val): 0.71    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input 64x64x3\n",
    "# YOUR CODE HERE\n",
    "class ImageClassifierNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3,padding=1)\n",
    "        #\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,padding=1)\n",
    "        #\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3,padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "       # self.conv7 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,padding=1)\n",
    "        \n",
    "        \n",
    "#         self.conv8 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3,padding=1)\n",
    "#         self.conv9 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3,padding=1)\n",
    "#         self.conv10 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3,padding=1)\n",
    "        \n",
    "#         self.conv11 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "     #   self.conv10_drop = nn.Dropout2d()\n",
    "    \n",
    "        self.fc1 = nn.Linear(8*8*256, 2)\n",
    "    #    self.fc2 = nn.Linear(150, 2)\n",
    "#         self.fc3 = nn.Linear(150, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #56   #32\n",
    "              \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #28   #16\n",
    "        \n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "      #  x = F.relu(self.conv7(x))     \n",
    "        x = F.max_pool2d(x, kernel_size=2) #14   #8\n",
    "        \n",
    "#         x = F.relu(self.conv11(x)) \n",
    "#         x = F.max_pool2d(x, kernel_size=2) #14   #4\n",
    "        \n",
    "#         x = F.relu(self.conv8(x))\n",
    "#         x = F.relu(self.conv9(x))\n",
    "#         x = F.relu(self.conv10(x)) \n",
    "#         x = F.relu(self.conv10_drop(x))\n",
    "#         x = F.max_pool2d(x, kernel_size=2) #4    #[14x14x512] if i=224x224    7x7x512 if i=112\n",
    "        \n",
    "   #     x = F.relu(self.conv11(x))\n",
    "   #     x = F.max_pool2d(x, kernel_size=2) #[7x7x512]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        x = x.view(-1, 8*8*256)\n",
    "      #  x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "       # x = F.dropout(x, training=self.training)\n",
    "        x = self.fc1(x)\n",
    "     #   return F.softmax(x, dim=1)\n",
    "        return x\n",
    "# Epoch 0 \tLoss: 0.692 \tLoss (val): 0.688\tAccuracy: 0.52 \tAccuracy (val): 0.53\n",
    "# Epoch 1 \tLoss: 0.688 \tLoss (val): 0.689\tAccuracy: 0.55 \tAccuracy (val): 0.52\n",
    "# Epoch 2 \tLoss: 0.680 \tLoss (val): 0.674\tAccuracy: 0.56 \tAccuracy (val): 0.58\n",
    "# Epoch 3 \tLoss: 0.650 \tLoss (val): 0.660\tAccuracy: 0.61 \tAccuracy (val): 0.62\n",
    "# Epoch 4 \tLoss: 0.617 \tLoss (val): 0.624\tAccuracy: 0.66 \tAccuracy (val): 0.65\n",
    "# Epoch 5 \tLoss: 0.580 \tLoss (val): 0.640\tAccuracy: 0.70 \tAccuracy (val): 0.63\n",
    "# Epoch 6 \tLoss: 0.561 \tLoss (val): 0.638\tAccuracy: 0.72 \tAccuracy (val): 0.63\n",
    "# Epoch 7 \tLoss: 0.513 \tLoss (val): 0.541\tAccuracy: 0.75 \tAccuracy (val): 0.71\n",
    "# Epoch 8 \tLoss: 0.473 \tLoss (val): 0.574\tAccuracy: 0.77 \tAccuracy (val): 0.69\n",
    "# Epoch 9 \tLoss: 0.445 \tLoss (val): 0.672\tAccuracy: 0.79 \tAccuracy (val): 0.68\n",
    "# Epoch 10 \tLoss: 0.423 \tLoss (val): 0.782\tAccuracy: 0.80 \tAccuracy (val): 0.66\n",
    "# Epoch 11 \tLoss: 0.394 \tLoss (val): 0.628\tAccuracy: 0.82 \tAccuracy (val): 0.70\n",
    "# Epoch 12 \tLoss: 0.346 \tLoss (val): 0.545\tAccuracy: 0.85 \tAccuracy (val): 0.74\n",
    "# Epoch 13 \tLoss: 0.336 \tLoss (val): 0.584\tAccuracy: 0.86 \tAccuracy (val): 0.73\n",
    "# Epoch 14 \tLoss: 0.279 \tLoss (val): 0.539\tAccuracy: 0.88 \tAccuracy (val): 0.76\n",
    "# Epoch 15 \tLoss: 0.237 \tLoss (val): 0.675\tAccuracy: 0.91 \tAccuracy (val): 0.74\n",
    "# Epoch 16 \tLoss: 0.193 \tLoss (val): 0.713\tAccuracy: 0.92 \tAccuracy (val): 0.73\n",
    "# Epoch 17 \tLoss: 0.148 \tLoss (val): 0.698\tAccuracy: 0.94 \tAccuracy (val): 0.74\n",
    "# Epoch 18 \tLoss: 0.108 \tLoss (val): 0.755\tAccuracy: 0.96 \tAccuracy (val): 0.76\n",
    "# Epoch 19 \tLoss: 0.089 \tLoss (val): 0.925\tAccuracy: 0.97 \tAccuracy (val): 0.75\n",
    "# Epoch 20 \tLoss: 0.053 \tLoss (val): 1.172\tAccuracy: 0.98 \tAccuracy (val): 0.75\n",
    "# Epoch 21 \tLoss: 0.047 \tLoss (val): 1.120\tAccuracy: 0.98 \tAccuracy (val): 0.75\n",
    "# Epoch 22 \tLoss: 0.023 \tLoss (val): 1.194\tAccuracy: 0.99 \tAccuracy (val): 0.74\n",
    "# Epoch 23 \tLoss: 0.013 \tLoss (val): 1.362\tAccuracy: 1.00 \tAccuracy (val): 0.74\n",
    "# Epoch 24 \tLoss: 0.008 \tLoss (val): 1.391\tAccuracy: 1.00 \tAccuracy (val): 0.76\n",
    "# Epoch 25 \tLoss: 0.021 \tLoss (val): 1.519\tAccuracy: 0.99 \tAccuracy (val): 0.72\n",
    "# Epoch 26 \tLoss: 0.097 \tLoss (val): 1.261\tAccuracy: 0.97 \tAccuracy (val): 0.72\n",
    "# Epoch 27 \tLoss: 0.080 \tLoss (val): 1.031\tAccuracy: 0.97 \tAccuracy (val): 0.75\n",
    "# Epoch 28 \tLoss: 0.027 \tLoss (val): 1.184\tAccuracy: 0.99 \tAccuracy (val): 0.76\n",
    "# Epoch 29 \tLoss: 0.010 \tLoss (val): 1.407\tAccuracy: 1.00 \tAccuracy (val): 0.76\n",
    "# Epoch 30 \tLoss: 0.003 \tLoss (val): 1.502\tAccuracy: 1.00 \tAccuracy (val): 0.76\n",
    "# Epoch 31 \tLoss: 0.001 \tLoss (val): 1.576\tAccuracy: 1.00 \tAccuracy (val): 0.76\n",
    "# Epoch 32 \tLoss: 0.000 \tLoss (val): 1.615\tAccuracy: 1.00 \tAccuracy (val): 0.76\n",
    "# Epoch 33 \tLoss: 0.000 \tLoss (val): 1.670\tAccuracy: 1.00 \tAccuracy (val): 0.76\n",
    "# Epoch 34 \tLoss: 0.000 \tLoss (val): 1.704\tAccuracy: 1.00 \tAccuracy (val): 0.76    \n",
    "\n",
    "\n",
    "\n",
    "# Epoch 0 \tLoss: 0.692 \tLoss (val): 0.692\tAccuracy: 0.52 \tAccuracy (val): 0.53\n",
    "# Epoch 1 \tLoss: 0.654 \tLoss (val): 0.666\tAccuracy: 0.61 \tAccuracy (val): 0.56\n",
    "# Epoch 2 \tLoss: 0.623 \tLoss (val): 0.670\tAccuracy: 0.65 \tAccuracy (val): 0.55\n",
    "# Epoch 3 \tLoss: 0.595 \tLoss (val): 0.643\tAccuracy: 0.68 \tAccuracy (val): 0.62\n",
    "# Epoch 4 \tLoss: 0.566 \tLoss (val): 0.629\tAccuracy: 0.71 \tAccuracy (val): 0.63\n",
    "# Epoch 5 \tLoss: 0.549 \tLoss (val): 0.617\tAccuracy: 0.71 \tAccuracy (val): 0.66\n",
    "# Epoch 6 \tLoss: 0.523 \tLoss (val): 0.590\tAccuracy: 0.75 \tAccuracy (val): 0.68\n",
    "# Epoch 7 \tLoss: 0.498 \tLoss (val): 0.647\tAccuracy: 0.76 \tAccuracy (val): 0.65\n",
    "# Epoch 8 \tLoss: 0.471 \tLoss (val): 0.607\tAccuracy: 0.78 \tAccuracy (val): 0.69\n",
    "# Epoch 9 \tLoss: 0.426 \tLoss (val): 0.578\tAccuracy: 0.80 \tAccuracy (val): 0.70\n",
    "# Epoch 10 \tLoss: 0.405 \tLoss (val): 0.588\tAccuracy: 0.82 \tAccuracy (val): 0.72\n",
    "# Epoch 11 \tLoss: 0.360 \tLoss (val): 0.586\tAccuracy: 0.85 \tAccuracy (val): 0.73\n",
    "# Epoch 12 \tLoss: 0.340 \tLoss (val): 0.581\tAccuracy: 0.86 \tAccuracy (val): 0.75\n",
    "# Epoch 13 \tLoss: 0.295 \tLoss (val): 0.619\tAccuracy: 0.88 \tAccuracy (val): 0.71\n",
    "# Epoch 14 \tLoss: 0.257 \tLoss (val): 0.575\tAccuracy: 0.89 \tAccuracy (val): 0.75\n",
    "# Epoch 15 \tLoss: 0.227 \tLoss (val): 0.851\tAccuracy: 0.91 \tAccuracy (val): 0.70\n",
    "# Epoch 16 \tLoss: 0.174 \tLoss (val): 0.705\tAccuracy: 0.93 \tAccuracy (val): 0.76\n",
    "# Epoch 17 \tLoss: 0.111 \tLoss (val): 0.846\tAccuracy: 0.96 \tAccuracy (val): 0.74\n",
    "# Epoch 18 \tLoss: 0.152 \tLoss (val): 0.886\tAccuracy: 0.93 \tAccuracy (val): 0.71\n",
    "# Epoch 19 \tLoss: 0.109 \tLoss (val): 1.138\tAccuracy: 0.96 \tAccuracy (val): 0.72\n",
    "# Epoch 20 \tLoss: 0.055 \tLoss (val): 1.162\tAccuracy: 0.98 \tAccuracy (val): 0.72\n",
    "# Epoch 21 \tLoss: 0.025 \tLoss (val): 1.156\tAccuracy: 0.99 \tAccuracy (val): 0.75\n",
    "# Epoch 22 \tLoss: 0.016 \tLoss (val): 1.689\tAccuracy: 1.00 \tAccuracy (val): 0.73\n",
    "# Epoch 23 \tLoss: 0.010 \tLoss (val): 1.870\tAccuracy: 1.00 \tAccuracy (val): 0.73\n",
    "# Epoch 24 \tLoss: 0.005 \tLoss (val): 1.697\tAccuracy: 1.00 \tAccuracy (val): 0.73\n",
    "# Epoch 25 \tLoss: 0.003 \tLoss (val): 1.988\tAccuracy: 1.00 \tAccuracy (val): 0.74\n",
    "# Epoch 26 \tLoss: 0.001 \tLoss (val): 2.085\tAccuracy: 1.00 \tAccuracy (val): 0.73\n",
    "# Epoch 27 \tLoss: 0.000 \tLoss (val): 2.104\tAccuracy: 1.00 \tAccuracy (val): 0.74\n",
    "# Epoch 28 \tLoss: 0.000 \tLoss (val): 2.128\tAccuracy: 1.00 \tAccuracy (val): 0.74\n",
    "# Epoch 29 \tLoss: 0.000 \tLoss (val): 2.185\tAccuracy: 1.00 \tAccuracy (val): 0.74\n",
    "# Epoch 30 \tLoss: 0.000 \tLoss (val): 2.205\tAccuracy: 1.00 \tAccuracy (val): 0.74\n",
    "# Epoch 31 \tLoss: 0.000 \tLoss (val): 2.259\tAccuracy: 1.00 \tAccuracy (val): 0.74\n",
    "# Epoch 32 \tLoss: 0.000 \tLoss (val): 2.283\tAccuracy: 1.00 \tAccuracy (val): 0.74\n",
    "# Epoch 33 \tLoss: 0.000 \tLoss (val): 2.311\tAccuracy: 1.00 \tAccuracy (val): 0.74\n",
    "# Epoch 34 \tLoss: 0.000 \tLoss (val): 2.342\tAccuracy: 1.00 \tAccuracy (val): 0.74\n",
    "\n",
    "\n",
    "# Epoch 0 \tLoss: 0.696 \tLoss (val): 0.689\tAccuracy: 0.50 \tAccuracy (val): 0.50\n",
    "# Epoch 1 \tLoss: 0.674 \tLoss (val): 0.667\tAccuracy: 0.58 \tAccuracy (val): 0.61\n",
    "# Epoch 2 \tLoss: 0.643 \tLoss (val): 0.719\tAccuracy: 0.63 \tAccuracy (val): 0.55\n",
    "# Epoch 3 \tLoss: 0.617 \tLoss (val): 0.658\tAccuracy: 0.66 \tAccuracy (val): 0.62\n",
    "# Epoch 4 \tLoss: 0.608 \tLoss (val): 0.606\tAccuracy: 0.67 \tAccuracy (val): 0.66\n",
    "# Epoch 5 \tLoss: 0.563 \tLoss (val): 0.653\tAccuracy: 0.71 \tAccuracy (val): 0.65\n",
    "# Epoch 6 \tLoss: 0.554 \tLoss (val): 0.598\tAccuracy: 0.72 \tAccuracy (val): 0.66\n",
    "# Epoch 7 \tLoss: 0.541 \tLoss (val): 0.587\tAccuracy: 0.72 \tAccuracy (val): 0.68\n",
    "# Epoch 8 \tLoss: 0.501 \tLoss (val): 0.622\tAccuracy: 0.74 \tAccuracy (val): 0.67\n",
    "# Epoch 9 \tLoss: 0.462 \tLoss (val): 0.553\tAccuracy: 0.78 \tAccuracy (val): 0.72\n",
    "# Epoch 10 \tLoss: 0.427 \tLoss (val): 0.583\tAccuracy: 0.80 \tAccuracy (val): 0.73\n",
    "# Epoch 11 \tLoss: 0.391 \tLoss (val): 0.616\tAccuracy: 0.82 \tAccuracy (val): 0.72\n",
    "# Epoch 12 \tLoss: 0.342 \tLoss (val): 0.628\tAccuracy: 0.85 \tAccuracy (val): 0.72\n",
    "# Epoch 13 \tLoss: 0.305 \tLoss (val): 0.565\tAccuracy: 0.87 \tAccuracy (val): 0.74\n",
    "# Epoch 14 \tLoss: 0.271 \tLoss (val): 0.670\tAccuracy: 0.89 \tAccuracy (val): 0.73\n",
    "# Epoch 15 \tLoss: 0.236 \tLoss (val): 0.677\tAccuracy: 0.91 \tAccuracy (val): 0.74\n",
    "# Epoch 16 \tLoss: 0.191 \tLoss (val): 0.719\tAccuracy: 0.93 \tAccuracy (val): 0.77\n",
    "# Epoch 17 \tLoss: 0.145 \tLoss (val): 0.809\tAccuracy: 0.95 \tAccuracy (val): 0.74\n",
    "# Epoch 18 \tLoss: 0.084 \tLoss (val): 1.535\tAccuracy: 0.97 \tAccuracy (val): 0.68\n",
    "# Epoch 19 \tLoss: 0.081 \tLoss (val): 1.149\tAccuracy: 0.97 \tAccuracy (val): 0.75\n",
    "# Epoch 20 \tLoss: 0.068 \tLoss (val): 1.012\tAccuracy: 0.98 \tAccuracy (val): 0.76\n",
    "# Epoch 21 \tLoss: 0.038 \tLoss (val): 1.166\tAccuracy: 0.99 \tAccuracy (val): 0.76\n",
    "# Epoch 22 \tLoss: 0.025 \tLoss (val): 1.401\tAccuracy: 0.99 \tAccuracy (val): 0.75\n",
    "# Epoch 23 \tLoss: 0.039 \tLoss (val): 1.257\tAccuracy: 0.99 \tAccuracy (val): 0.75\n",
    "# Epoch 24 \tLoss: 0.060 \tLoss (val): 1.316\tAccuracy: 0.98 \tAccuracy (val): 0.72\n",
    "# Epoch 25 \tLoss: 0.033 \tLoss (val): 1.375\tAccuracy: 0.99 \tAccuracy (val): 0.73\n",
    "# Epoch 26 \tLoss: 0.013 \tLoss (val): 1.404\tAccuracy: 1.00 \tAccuracy (val): 0.75\n",
    "# Epoch 27 \tLoss: 0.010 \tLoss (val): 1.795\tAccuracy: 1.00 \tAccuracy (val): 0.71\n",
    "# Epoch 28 \tLoss: 0.008 \tLoss (val): 1.757\tAccuracy: 1.00 \tAccuracy (val): 0.74\n",
    "# Epoch 29 \tLoss: 0.002 \tLoss (val): 1.884\tAccuracy: 1.00 \tAccuracy (val): 0.74\n",
    "# Epoch 30 \tLoss: 0.001 \tLoss (val): 1.775\tAccuracy: 1.00 \tAccuracy (val): 0.75\n",
    "# Epoch 31 \tLoss: 0.000 \tLoss (val): 1.902\tAccuracy: 1.00 \tAccuracy (val): 0.75\n",
    "# Epoch 32 \tLoss: 0.000 \tLoss (val): 2.054\tAccuracy: 1.00 \tAccuracy (val): 0.74\n",
    "# Epoch 33 \tLoss: 0.000 \tLoss (val): 2.086\tAccuracy: 1.00 \tAccuracy (val): 0.74\n",
    "# Epoch 34 \tLoss: 0.000 \tLoss (val): 2.247\tAccuracy: 1.00 \tAccuracy (val): 0.74\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "class ImageClassifierNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3,padding=1)\n",
    "        #\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,padding=1)\n",
    "        #\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,padding=1)\n",
    "        self.conv7 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,padding=1)\n",
    "        \n",
    "        \n",
    "#         self.conv8 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3,padding=1)\n",
    "#         self.conv9 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3,padding=1)\n",
    "#         self.conv10 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3,padding=1)\n",
    "        \n",
    "#         self.conv11 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "     #   self.conv10_drop = nn.Dropout2d()\n",
    "    \n",
    "        self.fc1 = nn.Linear(8*8*128, 150)\n",
    "        self.fc2 = nn.Linear(150, 2)\n",
    "#         self.fc3 = nn.Linear(150, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #56   #32\n",
    "              \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #28   #16\n",
    "        \n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = F.relu(self.conv7(x))     \n",
    "        x = F.max_pool2d(x, kernel_size=2) #14   #8\n",
    "        \n",
    "#         x = F.relu(self.conv11(x)) \n",
    "#         x = F.max_pool2d(x, kernel_size=2) #14   #4\n",
    "        \n",
    "#         x = F.relu(self.conv8(x))\n",
    "#         x = F.relu(self.conv9(x))\n",
    "#         x = F.relu(self.conv10(x)) \n",
    "#         x = F.relu(self.conv10_drop(x))\n",
    "#         x = F.max_pool2d(x, kernel_size=2) #4    #[14x14x512] if i=224x224    7x7x512 if i=112\n",
    "        \n",
    "   #     x = F.relu(self.conv11(x))\n",
    "   #     x = F.max_pool2d(x, kernel_size=2) #[7x7x512]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        x = x.view(-1, 8*8*128)\n",
    "        x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "       # x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "     #   return F.softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "# Epoch 0 \tLoss: 0.704 \tLoss (val): 0.693\tAccuracy: 0.50 \tAccuracy (val): 0.50\n",
    "# Epoch 1 \tLoss: 0.693 \tLoss (val): 0.693\tAccuracy: 0.50 \tAccuracy (val): 0.50\n",
    "# Epoch 2 \tLoss: 0.691 \tLoss (val): 0.749\tAccuracy: 0.53 \tAccuracy (val): 0.50\n",
    "# Epoch 3 \tLoss: 0.678 \tLoss (val): 0.672\tAccuracy: 0.57 \tAccuracy (val): 0.61\n",
    "# Epoch 4 \tLoss: 0.658 \tLoss (val): 0.689\tAccuracy: 0.61 \tAccuracy (val): 0.57\n",
    "# Epoch 5 \tLoss: 0.637 \tLoss (val): 0.667\tAccuracy: 0.64 \tAccuracy (val): 0.59\n",
    "# Epoch 6 \tLoss: 0.617 \tLoss (val): 0.654\tAccuracy: 0.66 \tAccuracy (val): 0.60\n",
    "# Epoch 7 \tLoss: 0.608 \tLoss (val): 0.652\tAccuracy: 0.68 \tAccuracy (val): 0.63\n",
    "# Epoch 8 \tLoss: 0.581 \tLoss (val): 0.690\tAccuracy: 0.69 \tAccuracy (val): 0.63\n",
    "# Epoch 9 \tLoss: 0.581 \tLoss (val): 0.668\tAccuracy: 0.69 \tAccuracy (val): 0.64\n",
    "# Epoch 10 \tLoss: 0.582 \tLoss (val): 0.615\tAccuracy: 0.69 \tAccuracy (val): 0.66\n",
    "# Epoch 11 \tLoss: 0.548 \tLoss (val): 0.596\tAccuracy: 0.72 \tAccuracy (val): 0.67\n",
    "# Epoch 12 \tLoss: 0.519 \tLoss (val): 0.589\tAccuracy: 0.75 \tAccuracy (val): 0.69\n",
    "# Epoch 13 \tLoss: 0.504 \tLoss (val): 0.594\tAccuracy: 0.76 \tAccuracy (val): 0.69\n",
    "# Epoch 14 \tLoss: 0.493 \tLoss (val): 0.549\tAccuracy: 0.77 \tAccuracy (val): 0.72\n",
    "# Epoch 15 \tLoss: 0.475 \tLoss (val): 0.561\tAccuracy: 0.77 \tAccuracy (val): 0.72\n",
    "# Epoch 16 \tLoss: 0.432 \tLoss (val): 0.564\tAccuracy: 0.80 \tAccuracy (val): 0.74\n",
    "# Epoch 17 \tLoss: 0.418 \tLoss (val): 0.614\tAccuracy: 0.80 \tAccuracy (val): 0.70\n",
    "# Epoch 18 \tLoss: 0.385 \tLoss (val): 0.667\tAccuracy: 0.82 \tAccuracy (val): 0.66\n",
    "# Epoch 19 \tLoss: 0.370 \tLoss (val): 0.618\tAccuracy: 0.83 \tAccuracy (val): 0.75\n",
    "# Epoch 20 \tLoss: 0.324 \tLoss (val): 0.665\tAccuracy: 0.86 \tAccuracy (val): 0.72\n",
    "# Epoch 21 \tLoss: 0.284 \tLoss (val): 0.739\tAccuracy: 0.88 \tAccuracy (val): 0.73\n",
    "# Epoch 22 \tLoss: 0.232 \tLoss (val): 0.741\tAccuracy: 0.90 \tAccuracy (val): 0.75\n",
    "# Epoch 23 \tLoss: 0.171 \tLoss (val): 0.872\tAccuracy: 0.93 \tAccuracy (val): 0.74\n",
    "# Epoch 24 \tLoss: 0.192 \tLoss (val): 0.982\tAccuracy: 0.93 \tAccuracy (val): 0.67\n",
    "# Epoch 25 \tLoss: 0.146 \tLoss (val): 0.988\tAccuracy: 0.95 \tAccuracy (val): 0.75\n",
    "# Epoch 26 \tLoss: 0.089 \tLoss (val): 1.107\tAccuracy: 0.97 \tAccuracy (val): 0.75\n",
    "# Epoch 27 \tLoss: 0.152 \tLoss (val): 0.850\tAccuracy: 0.94 \tAccuracy (val): 0.75\n",
    "# Epoch 28 \tLoss: 0.083 \tLoss (val): 1.227\tAccuracy: 0.97 \tAccuracy (val): 0.73\n",
    "# Epoch 29 \tLoss: 0.066 \tLoss (val): 1.340\tAccuracy: 0.98 \tAccuracy (val): 0.74\n",
    "# Epoch 30 \tLoss: 0.064 \tLoss (val): 1.118\tAccuracy: 0.97 \tAccuracy (val): 0.74\n",
    "# Epoch 31 \tLoss: 0.080 \tLoss (val): 1.167\tAccuracy: 0.97 \tAccuracy (val): 0.75\n",
    "# Epoch 32 \tLoss: 0.025 \tLoss (val): 1.632\tAccuracy: 0.99 \tAccuracy (val): 0.75\n",
    "# Epoch 33 \tLoss: 0.017 \tLoss (val): 1.905\tAccuracy: 1.00 \tAccuracy (val): 0.74\n",
    "# Epoch 34 \tLoss: 0.012 \tLoss (val): 1.885\tAccuracy: 1.00 \tAccuracy (val): 0.76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "class ImageClassifierNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3,padding=1)\n",
    "        #\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,padding=1)\n",
    "        #\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3,padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "        self.conv7 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "        \n",
    "        \n",
    "#         self.conv8 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3,padding=1)\n",
    "#         self.conv9 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3,padding=1)\n",
    "#         self.conv10 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3,padding=1)\n",
    "        \n",
    "#         self.conv11 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "     #   self.conv10_drop = nn.Dropout2d()\n",
    "    \n",
    "        self.fc1 = nn.Linear(8*8*256, 150)\n",
    "        self.fc2 = nn.Linear(150, 2)\n",
    "#         self.fc3 = nn.Linear(150, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #56   #32\n",
    "              \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #28   #16\n",
    "        \n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = F.relu(self.conv7(x))     \n",
    "        x = F.max_pool2d(x, kernel_size=2) #14   #8\n",
    "        \n",
    "#         x = F.relu(self.conv11(x)) \n",
    "#         x = F.max_pool2d(x, kernel_size=2) #14   #4\n",
    "        \n",
    "#         x = F.relu(self.conv8(x))\n",
    "#         x = F.relu(self.conv9(x))\n",
    "#         x = F.relu(self.conv10(x)) \n",
    "#         x = F.relu(self.conv10_drop(x))\n",
    "#         x = F.max_pool2d(x, kernel_size=2) #4    #[14x14x512] if i=224x224    7x7x512 if i=112\n",
    "        \n",
    "   #     x = F.relu(self.conv11(x))\n",
    "   #     x = F.max_pool2d(x, kernel_size=2) #[7x7x512]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        x = x.view(-1, 8*8*256)\n",
    "        x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "       # x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "     #   return F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "class ImageClassifierNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3,padding=1)\n",
    "        #\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,padding=1)\n",
    "        #\n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3,padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "        self.conv7 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "        \n",
    "        \n",
    "        self.conv8 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3,padding=1)\n",
    "        self.conv9 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3,padding=1)\n",
    "        self.conv10 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3,padding=1)\n",
    "        \n",
    "#         self.conv11 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3,padding=1)\n",
    "        self.conv10_drop = nn.Dropout2d()\n",
    "    \n",
    "        self.fc1 = nn.Linear(4*4*512, 150)\n",
    "        self.fc2 = nn.Linear(150, 150)\n",
    "        self.fc3 = nn.Linear(150, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #56   #32\n",
    "              \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #28   #16\n",
    "        \n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = F.relu(self.conv7(x))     \n",
    "        x = F.max_pool2d(x, kernel_size=2) #14   #8\n",
    "        \n",
    "        \n",
    "        x = F.relu(self.conv8(x))\n",
    "        x = F.relu(self.conv9(x))\n",
    "        x = F.relu(self.conv10(x)) \n",
    "        x = F.relu(self.conv10_drop(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2) #4    #[14x14x512] if i=224x224    7x7x512 if i=112\n",
    "        \n",
    "   #     x = F.relu(self.conv11(x))\n",
    "   #     x = F.max_pool2d(x, kernel_size=2) #[7x7x512]\n",
    "        \n",
    "        x = x.view(-1, 4*4*512)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc3(x)\n",
    "     #   return F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "class ImageClassifierNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3,padding=1)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,padding=1)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3,padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "        self.conv7 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)\n",
    "        \n",
    "        \n",
    "        self.conv8 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3,padding=1)\n",
    "        self.conv9 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3,padding=1)\n",
    "        self.conv10 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3,padding=1)\n",
    "        \n",
    "     #   self.conv11 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3,padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(4*4*512, 150)\n",
    "        self.fc2 = nn.Linear(150, 2)\n",
    "       # self.fc3 = nn.Linear(500, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2)  #32\n",
    "              \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2)  #16\n",
    "        \n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = F.relu(self.conv7(x))     \n",
    "        x = F.max_pool2d(x, kernel_size=2)  #8\n",
    "        \n",
    "        \n",
    "        x = F.relu(self.conv8(x))\n",
    "        x = F.relu(self.conv9(x))\n",
    "        x = F.relu(self.conv10(x))     \n",
    "        x = F.max_pool2d(x, kernel_size=2)  #4  #[14x14x512] if i=224x224    7x7x512 if i=112\n",
    "        \n",
    "   #     x = F.relu(self.conv11(x))\n",
    "   #     x = F.max_pool2d(x, kernel_size=2) #[7x7x512]\n",
    "        \n",
    "        x = x.view(-1, 4*4*512)\n",
    "        x = F.relu(self.fc1(x))\n",
    "      #  x = F.relu(self.fc2(x))\n",
    "        x = self.fc2(x)\n",
    "     #   return F.softmax(x, dim=1)\n",
    "        return x\n",
    "    \n",
    "# Epoch 0 \tLoss: 0.696 \tLoss (val): 0.693\tAccuracy: 0.50 \tAccuracy (val): 0.50\n",
    "# Epoch 1 \tLoss: 0.693 \tLoss (val): 0.693\tAccuracy: 0.50 \tAccuracy (val): 0.50\n",
    "# Epoch 2 \tLoss: 0.693 \tLoss (val): 0.693\tAccuracy: 0.50 \tAccuracy (val): 0.50\n",
    "# Epoch 3 \tLoss: 0.690 \tLoss (val): 0.693\tAccuracy: 0.55 \tAccuracy (val): 0.50\n",
    "# Epoch 4 \tLoss: 0.691 \tLoss (val): 0.694\tAccuracy: 0.53 \tAccuracy (val): 0.53\n",
    "# Epoch 5 \tLoss: 0.686 \tLoss (val): 0.776\tAccuracy: 0.55 \tAccuracy (val): 0.54\n",
    "# Epoch 6 \tLoss: 0.686 \tLoss (val): 0.692\tAccuracy: 0.56 \tAccuracy (val): 0.51\n",
    "# Epoch 7 \tLoss: 0.685 \tLoss (val): 0.758\tAccuracy: 0.55 \tAccuracy (val): 0.53\n",
    "# Epoch 8 \tLoss: 0.679 \tLoss (val): 0.669\tAccuracy: 0.59 \tAccuracy (val): 0.59\n",
    "# Epoch 9 \tLoss: 0.639 \tLoss (val): 0.659\tAccuracy: 0.64 \tAccuracy (val): 0.60\n",
    "# Epoch 10 \tLoss: 0.630 \tLoss (val): 0.661\tAccuracy: 0.65 \tAccuracy (val): 0.61\n",
    "# Epoch 11 \tLoss: 0.603 \tLoss (val): 0.662\tAccuracy: 0.67 \tAccuracy (val): 0.64\n",
    "# Epoch 12 \tLoss: 0.584 \tLoss (val): 0.637\tAccuracy: 0.70 \tAccuracy (val): 0.65\n",
    "# Epoch 13 \tLoss: 0.572 \tLoss (val): 0.625\tAccuracy: 0.71 \tAccuracy (val): 0.67\n",
    "# Epoch 14 \tLoss: 0.558 \tLoss (val): 0.612\tAccuracy: 0.73 \tAccuracy (val): 0.66\n",
    "# Epoch 15 \tLoss: 0.528 \tLoss (val): 0.577\tAccuracy: 0.73 \tAccuracy (val): 0.72\n",
    "# Epoch 16 \tLoss: 0.541 \tLoss (val): 0.596\tAccuracy: 0.74 \tAccuracy (val): 0.68\n",
    "# Epoch 17 \tLoss: 0.501 \tLoss (val): 0.575\tAccuracy: 0.76 \tAccuracy (val): 0.72\n",
    "# Epoch 18 \tLoss: 0.467 \tLoss (val): 0.576\tAccuracy: 0.78 \tAccuracy (val): 0.71\n",
    "# Epoch 19 \tLoss: 0.468 \tLoss (val): 0.649\tAccuracy: 0.78 \tAccuracy (val): 0.71\n",
    "# Epoch 20 \tLoss: 0.431 \tLoss (val): 0.554\tAccuracy: 0.80 \tAccuracy (val): 0.73\n",
    "# Epoch 21 \tLoss: 0.425 \tLoss (val): 0.573\tAccuracy: 0.80 \tAccuracy (val): 0.70\n",
    "# Epoch 22 \tLoss: 0.373 \tLoss (val): 0.588\tAccuracy: 0.84 \tAccuracy (val): 0.72\n",
    "# Epoch 23 \tLoss: 0.323 \tLoss (val): 0.618\tAccuracy: 0.85 \tAccuracy (val): 0.75\n",
    "# Epoch 24 \tLoss: 0.288 \tLoss (val): 0.608\tAccuracy: 0.88 \tAccuracy (val): 0.73\n",
    "# Epoch 25 \tLoss: 0.260 \tLoss (val): 0.753\tAccuracy: 0.89 \tAccuracy (val): 0.74\n",
    "# Epoch 26 \tLoss: 0.211 \tLoss (val): 0.734\tAccuracy: 0.92 \tAccuracy (val): 0.73\n",
    "# Epoch 27 \tLoss: 0.184 \tLoss (val): 0.650\tAccuracy: 0.93 \tAccuracy (val): 0.75\n",
    "# Epoch 28 \tLoss: 0.164 \tLoss (val): 0.923\tAccuracy: 0.94 \tAccuracy (val): 0.75\n",
    "# Epoch 29 \tLoss: 0.136 \tLoss (val): 1.010\tAccuracy: 0.95 \tAccuracy (val): 0.73\n",
    "# Epoch 30 \tLoss: 0.149 \tLoss (val): 0.742\tAccuracy: 0.94 \tAccuracy (val): 0.75\n",
    "# Epoch 31 \tLoss: 0.122 \tLoss (val): 1.006\tAccuracy: 0.95 \tAccuracy (val): 0.74\n",
    "# Epoch 32 \tLoss: 0.068 \tLoss (val): 1.126\tAccuracy: 0.98 \tAccuracy (val): 0.75\n",
    "# Epoch 33 \tLoss: 0.057 \tLoss (val): 1.240\tAccuracy: 0.98 \tAccuracy (val): 0.74\n",
    "# Epoch 34 \tLoss: 0.074 \tLoss (val): 1.240\tAccuracy: 0.97 \tAccuracy (val): 0.74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01447485bf51fbce22145e60e0fb8c93",
     "grade": true,
     "grade_id": "cell-6edb7d7e343ab14b",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "class ImageClassifierNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=20, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=20, out_channels=50, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(in_channels=50, out_channels=120, kernel_size=3)\n",
    "        self.conv4 = nn.Conv2d(in_channels=120, out_channels=120, kernel_size=3)\n",
    "        self.conv5 = nn.Conv2d(in_channels=120, out_channels=100, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(4*4*100, 150)\n",
    "        self.fc2 = nn.Linear(150, 2)\n",
    "       # self.fc3 = nn.Linear(500, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = x.view(-1, 4*4*100)\n",
    "        x = F.relu(self.fc1(x))\n",
    "      #  x = F.relu(self.fc2(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "        #return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba6a43ffd69fce436811d9311aedcede",
     "grade": false,
     "grade_id": "cell-d033937b5a8b9875",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Create two plots. In one of them, plot the loss in the training and the validation datasets. In the other one, plot the accuracy in the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88f944698dc9dc353e1933fe16b6de87",
     "grade": true,
     "grade_id": "cell-3df999674672de47",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "79d9c109d0360f12011edaf01804bb44",
     "grade": false,
     "grade_id": "cell-a827c39d9e652e52",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(2 POE)** Did your results improve? What problems did your improvements fix? Explain why, or why not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "951f6d4450c82df3800e38edd678a422",
     "grade": true,
     "grade_id": "cell-cbda4b585ad39ddc",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c06074522725240265e2182d83fda4e2",
     "grade": false,
     "grade_id": "cell-c67bcc4fbec1808e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Save your model](https://pytorch.org/tutorials/beginner/saving_loading_models.html) to disk (the architecture, weights and optimizer state). This is simply so you can use it again easily in the later parts of the notebook, without having to keep it in memory or re-training it. The actual file you create is not relevant to your submission. The code to save the model is given in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming that you called your model \"my_model\"\n",
    "torch.save(model.state_dict(), \"my_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ceb570afa39d746b3b5f132ecb5bc72e",
     "grade": false,
     "grade_id": "cell-25f9cc8d17491d0d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 4. Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "209c5d113dd1805f4e44dbebc1192473",
     "grade": false,
     "grade_id": "cell-cf9b347fc3ee9255",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**From now, training on CPU will not be feasible. If your computer has a GPU, try it out! Otherwise, now is the time to connect to your cloud instance**\n",
    "\n",
    "Now, instead of trying to come up with a good architecture for this task, we'll use the VGG16 architecture, but with the top layers removed (the fully connected layers + softmax). We'll substitute them with a single fully connected layer, and a classification layer that makes sense for our problem.\n",
    "\n",
    "However, this model has a very high capacity, and will probably suffer a lot from overfitting if we try to train it from scratch, using only our small subset of data. Instead, we'll start the optimization with the weights obtained after training VGG16 on the ImageNet dataset.\n",
    "\n",
    "Start by loading the *pretrained* VGG16 model, from the [torchvision.models](https://pytorch.org/docs/stable/torchvision/models.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "984428d972274a469334141d07c8666a",
     "grade": true,
     "grade_id": "cell-01ebc4c9c306b985",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\Mohammad/.cache\\torch\\checkpoints\\vgg16-397923af.pth\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 528M/528M [10:54<00:00, 845kB/s]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "import torchvision.models as models\n",
    "vgg_model = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f04d97f938d6f944741d657a0350e9da",
     "grade": false,
     "grade_id": "cell-faed8047ef25a60d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Create a new model with the layers you want to add on top of VGG.\n",
    "\n",
    "*Hint:*\n",
    "- You can access and modify the top layers of the VGG model with `vgg_model.classifier`, and the remaining layers with `vgg_model.features`.\n",
    "- You can get the number of output features of `vgg_model.features` with `vgg_model.classifier[0].in_features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "62829c2400b8ae993b346dd1a4ca68c0",
     "grade": true,
     "grade_id": "cell-56cb37360051a638",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Dropout(p=0.5, inplace=False)\n",
      "  (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (4): ReLU(inplace=True)\n",
      "  (5): Dropout(p=0.5, inplace=False)\n",
      "  (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      ")\n",
      "2\n",
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU(inplace=True)\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): ReLU(inplace=True)\n",
      "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU(inplace=True)\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace=True)\n",
      "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (13): ReLU(inplace=True)\n",
      "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): ReLU(inplace=True)\n",
      "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (18): ReLU(inplace=True)\n",
      "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (20): ReLU(inplace=True)\n",
      "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (22): ReLU(inplace=True)\n",
      "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (25): ReLU(inplace=True)\n",
      "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (27): ReLU(inplace=True)\n",
      "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (29): ReLU(inplace=True)\n",
      "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "4096\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "print(vgg_model.classifier)\n",
    "print(vgg_model.features[23].kernel_size)\n",
    "print(vgg_model.features)\n",
    "print(vgg_model.classifier[6].in_features)\n",
    "print(vgg_model.classifier[5].p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e10f4a98f6a80e4e74c0d5bf18dbc5f",
     "grade": false,
     "grade_id": "cell-d746f9eb61e3ea44",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now add the new model on top of VGG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "642f3cbea497868385adff16643091c4",
     "grade": true,
     "grade_id": "cell-76e4aad7fbcf5d05",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU(inplace=True)\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): ReLU(inplace=True)\n",
      "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU(inplace=True)\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace=True)\n",
      "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (13): ReLU(inplace=True)\n",
      "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): ReLU(inplace=True)\n",
      "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (18): ReLU(inplace=True)\n",
      "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (20): ReLU(inplace=True)\n",
      "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (22): ReLU(inplace=True)\n",
      "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (25): ReLU(inplace=True)\n",
      "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (27): ReLU(inplace=True)\n",
      "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (29): ReLU(inplace=True)\n",
      "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "vgg_model.classifier[6].out_features=2\n",
    "print(vgg_model.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0ff2dfca08a9262327d0ee7d0c58b2bd",
     "grade": false,
     "grade_id": "cell-f76d1a7f6280af0d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 4.1 Using VGG features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "78c524d5ed4e56c277ae5812e83d797d",
     "grade": false,
     "grade_id": "cell-270f8ec140ddfba3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now we're almost ready to train the new model. However, since the top layers of this architecture are being initialized randomly, it's sometimes possible for them to generate large gradients that can wreck the pretraining of the bottom layers. To avoid this, freeze all the VGG layers in your architecture (i.e. signal to the optimizer that these should not be changed during optimization) by setting the attribute `requires_grad` for all parameters `vgg_model.features` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b460fd8c5e4c535489d896457d6b22e0",
     "grade": true,
     "grade_id": "cell-bfb58ea46c31df0a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Freeze bottom\n",
    "# YOUR CODE HERE\n",
    "# Freeze training for all layers\n",
    "for param in vgg_model.features.parameters():\n",
    "    param.require_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d306e1a138f63e8c8c5eb90707013c9e",
     "grade": false,
     "grade_id": "cell-b508ede3d760a86b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Perform the transfer learning by training the top layers of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f0d719703f2da49ac4b86f4480c5080",
     "grade": true,
     "grade_id": "cell-5a025e60545ca151",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.53 GiB (GPU 0; 4.00 GiB total capacity; 2.13 GiB already allocated; 560.29 MiB free; 250.90 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-939e3a6ab776>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;31m# Compute predictions and losses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\ProgramData\\Anaconda3\\envs\\dml\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\ProgramData\\Anaconda3\\envs\\dml\\lib\\site-packages\\torchvision\\models\\vgg.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\ProgramData\\Anaconda3\\envs\\dml\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\ProgramData\\Anaconda3\\envs\\dml\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\ProgramData\\Anaconda3\\envs\\dml\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\ProgramData\\Anaconda3\\envs\\dml\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\ProgramData\\Anaconda3\\envs\\dml\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    338\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    339\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[1;32m--> 340\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.53 GiB (GPU 0; 4.00 GiB total capacity; 2.13 GiB already allocated; 560.29 MiB free; 250.90 MiB cached)"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "model = vgg_model\n",
    "model.to(device);\n",
    "#loss_fn = nn.NLLLoss()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.0001,momentum=0.5)\n",
    "\n",
    "\n",
    "def evaluate_model(val_data_loader, model, loss_fn):\n",
    "    losses = []\n",
    "    n_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for b_x, b_y in val_data_loader:\n",
    "            b_x=b_x.to(device)\n",
    "            b_y=b_y.to(device)\n",
    "            pred = model(b_x)\n",
    "            loss = loss_fn(pred, b_y)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            hard_preds = pred.argmax(dim=1)\n",
    "            n_correct += torch.sum(pred.argmax(dim=1) == b_y).item()\n",
    "        val_accuracy = n_correct/len(val_dataset)\n",
    "        val_avg_loss = sum(losses)/len(losses)    \n",
    "    \n",
    "    return val_accuracy, val_avg_loss\n",
    "\n",
    "\n",
    "TRANSFORM_TRAINIMG = transforms.Compose([\n",
    "    transforms.Resize(224),    \n",
    "    transforms.CenterCrop(224), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=small_train_mean,\n",
    "                         std =small_train_std )\n",
    "    ])\n",
    "TRANSFORM_VALIMG = transforms.Compose([\n",
    "    transforms.Resize(224),  \n",
    "    transforms.CenterCrop(224),   \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=small_val_std,\n",
    "                         std =small_val_std )\n",
    "    ])\n",
    "\n",
    "train_dataset= ImageFolder( root=\"train\", transform=TRANSFORM_TRAINIMG )\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, drop_last=False, num_workers=4)\n",
    "\n",
    "val_dataset= ImageFolder( root=\"val\", transform=TRANSFORM_VALIMG ) \n",
    "val_data_loader = DataLoader(val_dataset, batch_size=256, num_workers=4)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(35):\n",
    "    losses = []\n",
    "    n_correct = 0\n",
    "    for b_x, b_y in train_data_loader:\n",
    "        b_x=b_x.to(device)\n",
    "        b_y=b_y.to(device)\n",
    "        \n",
    "        # Compute predictions and losses\n",
    "        pred = model(b_x)\n",
    "        loss = loss_fn(pred, b_y)\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Count number of correct predictions\n",
    "        hard_preds = pred.argmax(dim=1)\n",
    "        n_correct += torch.sum(pred.argmax(dim=1) == b_y).item()\n",
    "\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()    \n",
    "        \n",
    "    # Compute accuracy and loss in the entire training set\n",
    "    train_accuracy = n_correct/len(train_dataset)\n",
    "    train_avg_loss = sum(losses)/len(losses)    \n",
    "    \n",
    "\n",
    "    # Compute accuracy and loss in the entire validation set\n",
    "   \n",
    "    val_accuracy, val_avg_loss = evaluate_model(val_data_loader, model, loss_fn)\n",
    "  \n",
    "    \n",
    "    \n",
    "    # Display metrics\n",
    "    display_str = 'Epoch {} '\n",
    "    display_str += '\\tLoss: {:.3f} '\n",
    "    display_str += '\\tLoss (val): {:.3f}'\n",
    "    display_str += '\\tAccuracy: {:.2f} '\n",
    "    display_str += '\\tAccuracy (val): {:.2f}'\n",
    "    print(display_str.format(epoch, train_avg_loss, val_avg_loss, train_accuracy, val_accuracy))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "34bf11c4a55f184ccc7d589c841b0be2",
     "grade": false,
     "grade_id": "cell-ad79e1aa5c4a6185",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Create two plots. In one of them, plot the loss in the training and the validation datasets. In the other one, plot the accuracy in the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "99986b7bbdfb6b78c25112751969d11f",
     "grade": true,
     "grade_id": "cell-f17c882b2a09dee7",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8c4634579a40b62a3db715b027fbd80a",
     "grade": false,
     "grade_id": "cell-779d477ffe1ebbf6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "How does the model perform, compared to the model obtained in step 3? Create one plot with the training accuracy and another with the validation accuracy of the two scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "873c045fa2e6f22815a90194ed2785f3",
     "grade": true,
     "grade_id": "cell-e3e3990ba39bea67",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9eb35f2f008149a41dee4dd98bb8007c",
     "grade": false,
     "grade_id": "cell-b84dd461d5ddcc8d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(2 POE)** Compare these results. Which approach worked best, starting from scratch or doing transfer learning? Reflect on whether your comparison is fair or not:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dce3df56e420472eafb032186728064f",
     "grade": true,
     "grade_id": "cell-f9e1a6a643946cd2",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "746937de51331942cb10b63d8892f29e",
     "grade": false,
     "grade_id": "cell-c8afb448c67da5f8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(1 POE)** What are the main differences between the ImageNet dataset and the Dogs vs Cats dataset we used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e1152bb9b59ad5cb8016d3ebe648e97",
     "grade": true,
     "grade_id": "cell-2be321b63232ae01",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4abd95aa268f813d682dd372f5f46839",
     "grade": false,
     "grade_id": "cell-71a8b8de004f6e57",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Optional (2 POE)** Even though there are considerable differences between these datasets, why is it that transfer learning is still a good idea?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9605a949f6c6b94fac8feb5a6bcbbcad",
     "grade": true,
     "grade_id": "cell-655d00face15a862",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "988d32c37c7135943f9775a37a511973",
     "grade": false,
     "grade_id": "cell-19785940b9624d2c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Optional (1 POE)** In which scenario would transfer learning be unsuitable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ce818a8e86ceb511348597d975b34016",
     "grade": true,
     "grade_id": "cell-e79df7472ff5506a",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b24258079f4e71e1842b78e479095117",
     "grade": false,
     "grade_id": "cell-111f2b1d28919293",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Save the model to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(VGG_model.state_dict(), \"trans_learning_top_only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab5bf17534c2ac6852d79e32793fdbf7",
     "grade": false,
     "grade_id": "cell-544a73726bebe121",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 4.2 Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "54080ee12529dc9ec0d2a887cc564243",
     "grade": false,
     "grade_id": "cell-1ee9ebc87fd3358e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now that we have a better starting point for the top layers, we can train the entire network. Unfreeze the bottom layers by resetting the `requires_grad` attribute to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5fa86d1e45984112d3d27953e63d312e",
     "grade": true,
     "grade_id": "cell-3918c2cdd9817f7e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# UnFreeze bottom\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c7437f0a564e0430e81490cf7c2dc9a5",
     "grade": false,
     "grade_id": "cell-80fa8c89f1b262f1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Fine tune the model by training all the layers.\n",
    "\n",
    "Hint:\n",
    "- Even though we do have a decent starting point for the optimization, it's still possible that a bad hyper-parameter choice wrecks the preinitialization. Make sure to use a small learning rate for this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71ce705f916d83ecec507903ac4a4c64",
     "grade": true,
     "grade_id": "cell-594c6039216461e5",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "de73952e2230136bd52c1c3b2c590896",
     "grade": false,
     "grade_id": "cell-5dc3e388a41da3ed",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "How does the model perform, compared to the model trained with frozen layers? Create one plot with the training accuracy and another with the validation accuracy of the two scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7b2e69e2ffc7f5bff07ba62225b4cee",
     "grade": true,
     "grade_id": "cell-7edb12ee397ec817",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23eacf2e2de74dba9a8baf7f4c1c0877",
     "grade": false,
     "grade_id": "cell-5dae528a81d5ff24",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(1 POE)** Did the model's performance improve? Why (why not)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "69dfca588131944b0e9825a1532de432",
     "grade": true,
     "grade_id": "cell-0f4a5edca490320e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4971aa0a2e159c1780dedfc5e78b7c15",
     "grade": false,
     "grade_id": "cell-4ed3967e4f6c5f7f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Save the model to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"trans_learning_full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42d6362e7a0f25fc579ad6e33f1b401b",
     "grade": false,
     "grade_id": "cell-56908ee1e60aa411",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 4.3 Improving the top model (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f74ac0be60b7253bfe604c521647a07",
     "grade": false,
     "grade_id": "cell-3c8d8e5ab949ee35",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Improve the architecture for the layers you add on top of VGG16. Try different ideas! When you're happy with one architecture, copy it in the cell below and train it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c69b20551001d0e612f3b9221dc7dbc",
     "grade": true,
     "grade_id": "cell-22d09c8401d84b61",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8fc346783e0e1a80bd856ca85bf5e744",
     "grade": false,
     "grade_id": "cell-48933baad6c5afeb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(1 POE)** How does the model perform, compared to the model trained in step 4.2? Create one plot with the training accuracy and another with the validation accuracy of the two scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b520759c1ceb8218d203dc9655d25361",
     "grade": true,
     "grade_id": "cell-7cb62a04916a848e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "84662189498e2454093c4a54a53716d6",
     "grade": false,
     "grade_id": "cell-8bbfa3e11e2dfff9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Save the model to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"best_trans_learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "49c722f31cfd70e995b6226c86584565",
     "grade": false,
     "grade_id": "cell-ad0efbac33de5a65",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 5. Final training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e460754d2c0f05f0e79ae982a3fe3d3",
     "grade": false,
     "grade_id": "cell-cf811afdac96843b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now we'll train the model that achieved the best performance so far using the entire dataset.\n",
    "\n",
    "**Note**: start the optimization with the weights you obtained training in the smaller subset, i.e. *not* from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4ad8b2d67a68a5afcf4c8645d3070550",
     "grade": false,
     "grade_id": "cell-3ae2a65188e4ac74",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "First, create two new data loaders, one for training samples and one for validation samples. This time, they'll load data from the folders for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94d6c3dc3c25680c53c2f2d4aef85b9d",
     "grade": true,
     "grade_id": "cell-64eaa83780f5eac9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "122af97053f34fc41312863aab715317",
     "grade": false,
     "grade_id": "cell-f3f79586de42561b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Train your model using the full data. This optimization might take a long time, so live plotting of some metrics is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27996ba3be6dddb4dfe2da1d0964f5ff",
     "grade": true,
     "grade_id": "cell-c7dd71a632b5f152",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d0ab46f558fb4b49f877ca0bae45376b",
     "grade": false,
     "grade_id": "cell-b1861d3a543c6386",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "How does the model perform now when trained on the entire dataset, compared to when only trained on the smaller subset of data? Create one plot with the training accuracy and another with the validation accuracy of the two scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "52bacfa672fbc7eca004c87d041e3411",
     "grade": true,
     "grade_id": "cell-ceaac6be60ce36a9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c6d931b0ea2198e83794788a4582de55",
     "grade": false,
     "grade_id": "cell-b38092b08c150e7d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**(2 POE)** What can you conclude from these plots? Did you expect what you observe in the plots, explain!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "22d5b2529d702c64919bef4e02ca308c",
     "grade": true,
     "grade_id": "cell-694a3fbb7f081da8",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "616d9047ba0c30d8343e48ecc58bd4d0",
     "grade": false,
     "grade_id": "cell-5e1ddfbfceb4d194",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 6. Evaluation on test set (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ba9f068757111d07c7e537904e333cf",
     "grade": false,
     "grade_id": "cell-a97630bf5d85363f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now we'll evaluate your final model, obtained in step 6, on the test set. As mentioned before, the samples in the test set are not labeled, so we can't compute any performance metrics ourselves. \n",
    "\n",
    "As a bit of fun and to inspire some friendly competition you may instead submit it to Kaggle for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2117c8c8e04d212a24f96d72d047526c",
     "grade": false,
     "grade_id": "cell-96a8fded54ed7011",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Compute the predictions for all samples in the test set according to your best model, and save it in a .csv file with the format expected by the competition.\n",
    "\n",
    "Hints:\n",
    "- There is a `sampleSubmission.csv` file included in the zip data. Take a look at it to better understand what is the expected format here.\n",
    "- `pathlib`'s `Path` class has a `glob` function, which returns the filenames of all files in a given path.\n",
    "- If you don't know how to create and write to files with Python, Google can help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e235a9ab5690a066143575414247f751",
     "grade": true,
     "grade_id": "cell-cc77ac7849f856e1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "97877f48922ebbd0c50829231a227ce5",
     "grade": false,
     "grade_id": "cell-faf8664f26ff7f4e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now that you created your submission file, submit it to Kaggle for evaluation. The [old competition](https://www.kaggle.com/c/dogs-vs-cats) does not allow submissions any more, but you can submit your file to the [new one](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition) via the \"Late submission\" button (they use the same data). The Kaggle CLI can be used as well. Kaggle evaluates your submission according to your log-loss score. Which score did you obtain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b8507722245d56a20dd6809091664f78",
     "grade": true,
     "grade_id": "cell-e951dcec64dec85d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e8a7f3a8236f43994efe29067d7237c2",
     "grade": false,
     "grade_id": "cell-dc362abcfef32eae",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "What was the username you used for this submission?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8cc61665c676edcd9192df3c15714aa3",
     "grade": true,
     "grade_id": "cell-d519532bb1f957c3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "**Your answer:** (fill in here)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
